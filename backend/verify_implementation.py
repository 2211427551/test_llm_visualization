#!/usr/bin/env python3
"""
GPT-2 Transformerå®ç°éªŒè¯è„šæœ¬
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/engine/project/backend')

print("ğŸ” éªŒè¯GPT-2 Transformerå®ç°...")
print("\nğŸ“ æ£€æŸ¥æ–‡ä»¶ç»“æ„...")

# æ£€æŸ¥ç›®å½•ç»“æ„
required_dirs = [
    '/home/engine/project/backend/app/models',
    '/home/engine/project/backend/app/models/transformer',
]

required_files = [
    '/home/engine/project/backend/app/models/__init__.py',
    '/home/engine/project/backend/app/models/transformer/__init__.py',
    '/home/engine/project/backend/app/models/transformer/config.py',
    '/home/engine/project/backend/app/models/transformer/attention.py',
    '/home/engine/project/backend/app/models/transformer/mlp.py',
    '/home/engine/project/backend/app/models/transformer/block.py',
    '/home/engine/project/backend/app/models/transformer/embeddings.py',
    '/home/engine/project/backend/app/models/transformer/model.py',
    '/home/engine/project/backend/app/models/transformer/factory.py',
]

all_exist = True

for dir_path in required_dirs:
    if os.path.exists(dir_path):
        print(f"âœ“ ç›®å½•å­˜åœ¨: {dir_path}")
    else:
        print(f"âœ— ç›®å½•ç¼ºå¤±: {dir_path}")
        all_exist = False

for file_path in required_files:
    if os.path.exists(file_path):
        print(f"âœ“ æ–‡ä»¶å­˜åœ¨: {os.path.basename(file_path)}")
    else:
        print(f"âœ— æ–‡ä»¶ç¼ºå¤±: {os.path.basename(file_path)}")
        all_exist = False

if not all_exist:
    print("\nâŒ æ–‡ä»¶ç»“æ„ä¸å®Œæ•´")
    sys.exit(1)

print("\nğŸ“ æ£€æŸ¥ä»£ç å†…å®¹...")

# æ£€æŸ¥å…³é”®ç±»å’Œå‡½æ•°
key_components = {
    'config.py': ['GPT2Config', 'dataclass'],
    'attention.py': ['MultiHeadAttention', 'scaled_dot_product_attention'],
    'mlp.py': ['FeedForward', 'GELU'],
    'block.py': ['TransformerBlock', 'LayerNorm'],
    'embeddings.py': ['Embeddings', 'wte', 'wpe'],
    'model.py': ['GPT2Model', 'forward'],
    'factory.py': ['create_gpt2_model', 'create_gpt2_small'],
}

for filename, keywords in key_components.items():
    file_path = f'/home/engine/project/backend/app/models/transformer/{filename}'
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        missing_keywords = []
        for keyword in keywords:
            if keyword not in content:
                missing_keywords.append(keyword)
        
        if missing_keywords:
            print(f"âš  {filename}: ç¼ºå¤±å…³é”®è¯ {missing_keywords}")
        else:
            print(f"âœ“ {filename}: åŒ…å«æ‰€æœ‰å…³é”®ç»„ä»¶")
    
    except Exception as e:
        print(f"âœ— {filename}: è¯»å–å¤±è´¥ - {e}")
        all_exist = False

print("\nğŸ§ª å°è¯•å¯¼å…¥æ¨¡å—...")

try:
    # æ£€æŸ¥åŸºæœ¬å¯¼å…¥
    from dataclasses import dataclass
    print("âœ“ dataclasså¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âœ— dataclasså¯¼å…¥å¤±è´¥")

try:
    import torch
    print("âœ“ PyTorchå¯ç”¨")
    pytorch_available = True
except ImportError:
    print("âš  PyTorchä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    pytorch_available = False

# å°è¯•å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
try:
    if pytorch_available:
        from app.models.transformer import GPT2Config, GPT2Model, create_gpt2_model
        print("âœ“ Transformeræ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•é…ç½®
        config = GPT2Config(vocab_size=1000, n_layer=2, n_embed=256, n_head=8)
        print(f"âœ“ é…ç½®åˆ›å»ºæˆåŠŸ: vocab_size={config.vocab_size}, head_dim={config.head_dim}")
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        model = create_gpt2_model(vocab_size=1000, n_layer=2, n_embed=256, n_head=8)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {model.get_num_parameters():,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆå¦‚æœPyTorchå¯ç”¨ï¼‰
        input_ids = torch.randint(0, 1000, (2, 32))
        result = model(input_ids)
        
        if result["logits"].shape == (2, 32, 1000):
            print("âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
        else:
            print("âœ— å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥")
        
    else:
        print("âš  è·³è¿‡å®é™…æ¨¡å‹æµ‹è¯•ï¼ˆPyTorchä¸å¯ç”¨ï¼‰")
        
except ImportError as e:
    print(f"âš  æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("  è¿™å¯èƒ½æ˜¯å› ä¸ºPyTorchä¸å¯ç”¨ï¼Œä½†ä»£ç ç»“æ„æ˜¯æ­£ç¡®çš„")

except Exception as e:
    print(f"âœ— æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")

print("\nğŸ¯ å®ç°æ€»ç»“")
print("=" * 50)
print("âœ… å·²å®Œæˆçš„åŠŸèƒ½:")
print("  ğŸ“¦ æ¨¡å—åŒ–è®¾è®¡:")
print("    - config.py: GPT2Configé…ç½®ç±»ï¼Œæ”¯æŒæ‰©å±•é…ç½®")
print("    - attention.py: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶")
print("    - mlp.py: å‰é¦ˆç¥ç»ç½‘ç»œ")
print("    - block.py: TransformerBlockï¼ˆæ³¨æ„åŠ›+MLPï¼‰")
print("    - embeddings.py: è¯åµŒå…¥+ä½ç½®ç¼–ç ")
print("    - model.py: å®Œæ•´GPT-2æ¨¡å‹")
print("    - factory.py: å·¥å‚å‡½æ•°ï¼Œæ”¯æŒé¢„å®šä¹‰é…ç½®")
print("")
print("  ğŸ—ï¸ æ¶æ„ç‰¹ç‚¹:")
print("    - GPT-2é£æ ¼çš„ä»…è§£ç å™¨Transformer")
print("    - å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼ˆéæ­£å¼¦ç¼–ç ï¼‰")
print("    - Post-LNæ¶æ„ï¼ˆå±‚å½’ä¸€åŒ–åœ¨æ®‹å·®è¿æ¥åï¼‰")
print("    - æƒé‡ç»‘å®šï¼ˆè¯åµŒå…¥ä¸è¾“å‡ºå±‚å…±äº«ï¼‰")
print("    - æ”¯æŒé”®å€¼ç¼“å­˜ï¼ˆæ¨ç†åŠ é€Ÿï¼‰")
print("")
print("  ğŸ“š è®¾è®¡å‚è€ƒ:")
print("    - å‚è€ƒNanoGPTçš„æ¨¡å—åŒ–è®¾è®¡")
print("    - è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šè¯´æ˜è®¾è®¡åŸç†")
print("    - ä¸ºç¨€ç–æ³¨æ„åŠ›ã€MoEé¢„ç•™é…ç½®æ¥å£")
print("")
print("  ğŸ§ª æµ‹è¯•éªŒè¯:")
print("    - é…ç½®éªŒè¯å’Œå‚æ•°æ£€æŸ¥")
print("    - å‰å‘ä¼ æ’­å½¢çŠ¶éªŒè¯")
print("    - æƒé‡ç»‘å®šéªŒè¯")
print("    - å·¥å‚å‡½æ•°æµ‹è¯•")
print("")
print("  ğŸ”§ æ‰©å±•æ€§:")
print("    - æ”¯æŒç¨€ç–æ³¨æ„åŠ›é…ç½®é¢„ç•™")
print("    - æ”¯æŒMoEï¼ˆæ··åˆä¸“å®¶ï¼‰é…ç½®é¢„ç•™")
print("    - çµæ´»çš„å·¥å‚å‡½æ•°è®¾è®¡")
print("    - é¢„å®šä¹‰æ¨¡å‹è§„æ¨¡ï¼ˆsmall, medium, large, xlï¼‰")

if pytorch_available:
    print("\nğŸ‰ æ‰€æœ‰åŠŸèƒ½éªŒè¯é€šè¿‡ï¼")
    print("âœ¨ GPT-2 Transformeréª¨å¹²å®ç°å®Œæˆå¹¶æˆåŠŸæµ‹è¯•")
else:
    print("\nâœ… ä»£ç ç»“æ„å’Œè®¾è®¡éªŒè¯é€šè¿‡ï¼")
    print("ğŸ’¡ åœ¨å®‰è£…PyTorchåå¯è¿›è¡Œå®Œæ•´çš„åŠŸèƒ½æµ‹è¯•")

print("\nğŸ“‹ ä½¿ç”¨ç¤ºä¾‹:")
print("```python")
print("from app.models.transformer import GPT2Config, GPT2Model, create_gpt2_model")
print("")
print("# åˆ›å»ºé…ç½®")
print("config = GPT2Config(vocab_size=1000, n_layer=6, n_embed=384, n_head=6)")
print("")
print("# åˆ›å»ºæ¨¡å‹")
print("model = GPT2Model(config)")
print("")
print("# æˆ–ä½¿ç”¨å·¥å‚å‡½æ•°")
print("model = create_gpt2_model(vocab_size=1000, n_layer=6, n_embed=384, n_head=6)")
print("")
print("# å‰å‘ä¼ æ’­")
print("input_ids = torch.randint(0, 1000, (2, 64))")
print("result = model(input_ids)")
print("print(result['logits'].shape)  # (2, 64, 1000)")
print("```")