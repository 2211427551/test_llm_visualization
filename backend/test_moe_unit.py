"""
MoEå±‚å•å…ƒæµ‹è¯•

æµ‹è¯•Mixture of Expertså±‚çš„å„ç§åŠŸèƒ½ï¼š
1. Top-kè·¯ç”±æ­£ç¡®æ€§
2. æƒé‡å½’ä¸€åŒ–
3. æ¢¯åº¦åå‘ä¼ æ’­
4. è´Ÿè½½å‡è¡¡
5. ä¸­é—´æ•°æ®æ•è·
"""

import torch
import torch.nn as nn
from app.models.transformer.config import GPT2Config
from app.models.transformer.moe import MoELayer, MoEExpert, GatingNetwork
from app.models.transformer.block import TransformerBlock


class TestMoEExpert:
    """æµ‹è¯•MoEä¸“å®¶ç½‘ç»œ"""
    
    def test_expert_forward(self):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œå‰å‘ä¼ æ’­"""
        config = GPT2Config(n_embed=256, ffn_hidden_multiplier=4)
        expert = MoEExpert(config)
        
        batch_size, seq_len = 2, 10
        x = torch.randn(batch_size, seq_len, config.n_embed)
        
        output = expert(x)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert output.shape == (batch_size, seq_len, config.n_embed)
        
        # éªŒè¯è¾“å‡ºä¸æ˜¯NaNæˆ–Inf
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
    
    def test_expert_different_activations(self):
        """æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°"""
        activations = ["gelu", "relu", "swish", "tanh"]
        
        for activation in activations:
            config = GPT2Config(
                n_embed=128, 
                ffn_hidden_multiplier=4,
                moe_activation=activation
            )
            expert = MoEExpert(config)
            
            x = torch.randn(1, 5, config.n_embed)
            output = expert(x)
            
            assert output.shape == (1, 5, config.n_embed)
            assert not torch.isnan(output).any()


class TestGatingNetwork:
    """æµ‹è¯•Gatingç½‘ç»œ"""
    
    def test_gating_forward(self):
        """æµ‹è¯•é—¨æ§ç½‘ç»œå‰å‘ä¼ æ’­"""
        n_embed = 256
        num_experts = 8
        
        gating = GatingNetwork(n_embed, num_experts)
        
        batch_size, seq_len = 2, 10
        x = torch.randn(batch_size, seq_len, n_embed)
        
        gate_scores = gating(x)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert gate_scores.shape == (batch_size, seq_len, num_experts)
        
        # éªŒè¯æ¦‚ç‡å½’ä¸€åŒ–ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
        row_sums = gate_scores.sum(dim=-1)
        assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6)
        
        # éªŒè¯æ¦‚ç‡èŒƒå›´åœ¨[0, 1]
        assert (gate_scores >= 0).all()
        assert (gate_scores <= 1).all()


class TestMoELayer:
    """æµ‹è¯•MoEå±‚"""
    
    def test_moe_forward(self):
        """æµ‹è¯•MoEå±‚å‰å‘ä¼ æ’­"""
        config = GPT2Config(n_embed=256, ffn_hidden_multiplier=4)
        moe = MoELayer(config, num_experts=4, top_k=2)
        
        batch_size, seq_len = 2, 10
        x = torch.randn(batch_size, seq_len, config.n_embed)
        
        output, intermediate = moe(x, return_intermediate=True)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert output.shape == (batch_size, seq_len, config.n_embed)
        
        # éªŒè¯ä¸­é—´æ•°æ®
        assert intermediate is not None
        assert 'gate_scores' in intermediate
        assert 'top_k_scores' in intermediate
        assert 'top_k_indices' in intermediate
        assert 'expert_outputs' in intermediate
        assert 'final_output' in intermediate
        assert 'load_balance_loss' in intermediate
        
        # éªŒè¯é—¨æ§åˆ†æ•°å½¢çŠ¶
        assert intermediate['gate_scores'].shape == (batch_size, seq_len, 4)
        
        # éªŒè¯top-kåˆ†æ•°å’Œç´¢å¼•å½¢çŠ¶
        assert intermediate['top_k_scores'].shape == (batch_size, seq_len, 2)
        assert intermediate['top_k_indices'].shape == (batch_size, seq_len, 2)
    
    def test_top_k_routing(self):
        """æµ‹è¯•Top-kè·¯ç”±æ­£ç¡®æ€§"""
        config = GPT2Config(n_embed=128)
        num_experts = 4
        top_k = 2
        
        moe = MoELayer(config, num_experts=num_experts, top_k=top_k)
        
        batch_size, seq_len = 1, 3
        x = torch.randn(batch_size, seq_len, config.n_embed)
        
        output, intermediate = moe(x, return_intermediate=True)
        
        gate_scores = intermediate['gate_scores']
        top_k_scores = intermediate['top_k_scores']
        top_k_indices = intermediate['top_k_indices']
        
        # éªŒè¯top-kåˆ†æ•°ç¡®å®æ˜¯æœ€å¤§çš„kä¸ªå€¼
        for i in range(seq_len):
            expected_scores, expected_indices = torch.topk(
                gate_scores[0, i], k=top_k, sorted=True
            )
            assert torch.allclose(top_k_scores[0, i], expected_scores, atol=1e-6)
            assert torch.equal(top_k_indices[0, i], expected_indices)
    
    def test_weight_normalization(self):
        """æµ‹è¯•æƒé‡å½’ä¸€åŒ–"""
        config = GPT2Config(n_embed=128)
        moe = MoELayer(config, num_experts=4, top_k=2)
        
        x = torch.randn(1, 5, config.n_embed)
        _, intermediate = moe(x, return_intermediate=True)
        
        top_k_scores = intermediate['top_k_scores']
        
        # éªŒè¯æ¯è¡Œçš„top-kåˆ†æ•°å’Œä¸º1
        row_sums = top_k_scores.sum(dim=-1)
        assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6)
    
    def test_gradient_flow(self):
        """æµ‹è¯•æ¢¯åº¦åå‘ä¼ æ’­"""
        config = GPT2Config(n_embed=128)
        moe = MoELayer(config, num_experts=4, top_k=2)
        
        x = torch.randn(2, 3, config.n_embed, requires_grad=True)
        target = torch.randn(2, 3, config.n_embed)
        
        output, _ = moe(x, return_intermediate=True)
        loss = nn.MSELoss()(output, target)
        loss.backward()
        
        # éªŒè¯è¾“å…¥æ¢¯åº¦å­˜åœ¨
        assert x.grad is not None
        assert not torch.isnan(x.grad).any()
        
        # éªŒè¯æ¨¡å‹å‚æ•°æ¢¯åº¦å­˜åœ¨
        for param in moe.parameters():
            assert param.grad is not None
            assert not torch.isnan(param.grad).any()
    
    def test_load_balance_loss(self):
        """æµ‹è¯•è´Ÿè½½å‡è¡¡æŸå¤±"""
        config = GPT2Config(n_embed=128)
        moe = MoELayer(config, num_experts=4, top_k=2)
        
        x = torch.randn(2, 10, config.n_embed)
        _, intermediate = moe(x, return_intermediate=True)
        
        load_balance_loss = intermediate['load_balance_loss']
        
        # éªŒè¯æŸå¤±æ˜¯æ ‡é‡ä¸”éè´Ÿ
        assert load_balance_loss.dim() == 0
        assert load_balance_loss >= 0
    
    def test_expert_usage_stats(self):
        """æµ‹è¯•ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡"""
        config = GPT2Config(n_embed=128)
        moe = MoELayer(config, num_experts=4, top_k=2)
        
        x = torch.randn(2, 10, config.n_embed)
        _, intermediate = moe(x, return_intermediate=True)
        
        gate_scores = intermediate['gate_scores']
        stats = moe.get_expert_usage_stats(gate_scores)
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert 'expert_usage' in stats
        assert 'expert_selections' in stats
        assert 'usage_std' in stats
        assert 'selections_std' in stats
        
        # éªŒè¯å½¢çŠ¶
        assert stats['expert_usage'].shape == (4,)
        assert stats['expert_selections'].shape == (4,)
    
    def test_different_configurations(self):
        """æµ‹è¯•ä¸åŒé…ç½®çš„MoEå±‚"""
        configs = [
            {'num_experts': 2, 'top_k': 1},
            {'num_experts': 8, 'top_k': 2},
            {'num_experts': 16, 'top_k': 4},
        ]
        
        for config_dict in configs:
            config = GPT2Config(n_embed=128)
            moe = MoELayer(config, **config_dict)
            
            x = torch.randn(1, 5, config.n_embed)
            output, _ = moe(x)
            
            assert output.shape == (1, 5, config.n_embed)


class TestTransformerBlockWithMoE:
    """æµ‹è¯•é›†æˆMoEçš„TransformerBlock"""
    
    def test_block_with_moe(self):
        """æµ‹è¯•ä½¿ç”¨MoEçš„TransformerBlock"""
        config = GPT2Config(
            n_embed=256,
            n_head=8,
            n_layer=1,
            use_moe=True,
            moe_num_experts=4,
            moe_top_k=2
        )
        
        block = TransformerBlock(config)
        
        batch_size, seq_len = 2, 10
        x = torch.randn(batch_size, seq_len, config.n_embed)
        
        output, cache, intermediate = block(
            x, 
            use_cache=False, 
            return_intermediate=True
        )
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert output.shape == (batch_size, seq_len, config.n_embed)
        
        # éªŒè¯MoEä¸­é—´æ•°æ®
        assert intermediate is not None
        assert 'moe' in intermediate
        assert 'gate_scores' in intermediate['moe']
        assert 'top_k_indices' in intermediate['moe']
    
    def test_block_without_moe(self):
        """æµ‹è¯•ä¸ä½¿ç”¨MoEçš„TransformerBlock"""
        config = GPT2Config(
            n_embed=256,
            n_head=8,
            n_layer=1,
            use_moe=False
        )
        
        block = TransformerBlock(config)
        
        batch_size, seq_len = 2, 10
        x = torch.randn(batch_size, seq_len, config.n_embed)
        
        output, cache, intermediate = block(
            x, 
            use_cache=False, 
            return_intermediate=True
        )
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert output.shape == (batch_size, seq_len, config.n_embed)
        
        # éªŒè¯æ²¡æœ‰MoEä¸­é—´æ•°æ®
        if intermediate is not None:
            assert 'moe' not in intermediate
    
    def test_moe_vs_standard_ffn(self):
        """æ¯”è¾ƒMoEå’Œæ ‡å‡†FFNçš„è¾“å‡ºå·®å¼‚"""
        config_moe = GPT2Config(
            n_embed=128,
            n_head=4,
            use_moe=True,
            moe_num_experts=4,
            moe_top_k=2
        )
        
        config_standard = GPT2Config(
            n_embed=128,
            n_head=4,
            use_moe=False
        )
        
        block_moe = TransformerBlock(config_moe)
        block_standard = TransformerBlock(config_standard)
        
        x = torch.randn(1, 5, config_moe.n_embed)
        
        output_moe, _, _ = block_moe(x, return_intermediate=True)
        output_standard, _, _ = block_standard(x, return_intermediate=True)
        
        # è¾“å‡ºåº”è¯¥ä¸åŒï¼ˆMoEå’Œæ ‡å‡†FFNæœ‰ä¸åŒçš„è®¡ç®—è·¯å¾„ï¼‰
        assert not torch.allclose(output_moe, output_standard, atol=1e-6)
        
        # ä½†è¾“å‡ºå½¢çŠ¶åº”è¯¥ç›¸åŒ
        assert output_moe.shape == output_standard.shape


if __name__ == "__main__":
    # è¿è¡ŒåŸºæœ¬æµ‹è¯•
    print("å¼€å§‹è¿è¡ŒMoEå•å…ƒæµ‹è¯•...")
    
    test_expert = TestMoEExpert()
    test_expert.test_expert_forward()
    test_expert.test_expert_different_activations()
    print("âœ“ MoEä¸“å®¶ç½‘ç»œæµ‹è¯•é€šè¿‡")
    
    test_gating = TestGatingNetwork()
    test_gating.test_gating_forward()
    print("âœ“ Gatingç½‘ç»œæµ‹è¯•é€šè¿‡")
    
    test_moe = TestMoELayer()
    test_moe.test_moe_forward()
    test_moe.test_top_k_routing()
    test_moe.test_weight_normalization()
    test_moe.test_gradient_flow()
    test_moe.test_load_balance_loss()
    test_moe.test_expert_usage_stats()
    test_moe.test_different_configurations()
    print("âœ“ MoEå±‚æµ‹è¯•é€šè¿‡")
    
    test_block = TestTransformerBlockWithMoE()
    test_block.test_block_with_moe()
    test_block.test_block_without_moe()
    test_block.test_moe_vs_standard_ffn()
    print("âœ“ TransformerBlocké›†æˆæµ‹è¯•é€šè¿‡")
    
    print("\nğŸ‰ æ‰€æœ‰MoEæµ‹è¯•é€šè¿‡ï¼")