#!/usr/bin/env python3
"""
ç®€å•çš„å‰å‘æ•°æ®æ•è·æµ‹è¯•è„šæœ¬

ä¸ä¾èµ–pytestï¼Œç›´æ¥æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/home/engine/project/backend')

try:
    import torch
    print("âœ“ PyTorchå¯¼å…¥æˆåŠŸ")
    TORCH_AVAILABLE = True
except ImportError:
    print("âœ— PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    TORCH_AVAILABLE = False
    
    # åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ
    from dataclasses import dataclass
    from typing import List, Dict, Any, Optional
    import json
    from datetime import datetime
    
    class MockTensor:
        def __init__(self, shape, dtype="float32"):
            self.shape = shape
            self.dtype = dtype
            self.device = "cpu"
        
        def size(self):
            return self.shape
        
        def numel(self):
            result = 1
            for dim in self.shape:
                result *= dim
            return result
        
        def detach(self):
            return MockTensor(self.shape, self.dtype)
        
        def cpu(self):
            return MockTensor(self.shape, self.dtype)
        
        def numpy(self):
            import numpy as np
            return np.zeros(self.shape)
        
        def max(self):
            return MockTensor([])
        
        def min(self):
            return MockTensor([])
        
        def mean(self):
            return MockTensor([])
        
        def item(self):
            return 1.0
        
        def sum(self, dim=None):
            if dim is None:
                return MockTensor([])
            else:
                new_shape = list(self.shape)
                if isinstance(dim, int):
                    new_shape[dim] = 1
                return MockTensor(new_shape)
        
        def any(self):
            return False
        
        def mean(self, dim=None):
            return MockTensor([])
        
        def __getitem__(self, key):
            if isinstance(key, slice):
                return MockTensor(list(self.shape)[key])
            return MockTensor([])
        
        def expansion(self, *sizes):
            return MockTensor(sizes)
        
        def unsqueeze(self, dim):
            new_shape = list(self.shape)
            new_shape.insert(dim, 1)
            return MockTensor(new_shape)
        
        def expand(self, *sizes):
            return MockTensor(list(sizes))
        
        def element_size(self):
            return 4  # float32
        
        @property
        def data(self):
            return self
        
        def normal_(self, mean=0.0, std=1.0):
            return self
        
        def zeros_(self):
            return self
        
        def ones_(self):
            return self
    
    class MockModule:
        def __init__(self):
            pass
        
        def parameters(self):
            return [MockTensor([100, 200]), MockTensor([200, 300])]
        
        def eval(self):
            pass
        
        def train(self):
            pass
        
        def zero_grad(self):
            pass
    
    # æ¨¡æ‹Ÿtorch
    class MockTorch:
        Tensor = MockTensor
        
        @staticmethod
        def randint(low, high, size):
            return MockTensor(list(size))
        
        @staticmethod
        def randn(*size):
            return MockTensor(list(size))
        
        @staticmethod
        def empty(*size):
            return MockTensor(list(size))
        
        @staticmethod
        def allclose(a, b, atol=1e-6):
            return True
        
        @staticmethod
        def arange(n, device=None):
            return MockTensor([n])
        
        class device:
            def __init__(self, device_str):
                self.type = device_str.split(':')[0] if ':' in device_str else device_str
        
        @staticmethod
        def softmax(input, dim):
            return MockTensor(input.shape)
        
        @staticmethod
        def zeros_like(input):
            return MockTensor(input.shape)
        
        @staticmethod
        def var(input, dim=None):
            return MockTensor([])
        
        @staticmethod
        def topk(input, k, dim=-1, sorted=True):
            return MockTensor([*input.shape[:-1], k]), MockTensor([*input.shape[:-1], k])
        
        class no_grad:
            def __enter__(self):
                return self
            
            def __exit__(self, exc_type, exc_val, exc_tb):
                pass
        
        class nn:
            class Module:
                def __init__(self):
                    pass
                
                def eval(self):
                    pass
                
                def train(self):
                    pass
                
                def apply(self, fn):
                    # Apply function to all submodules
                    for attr_name in dir(self):
                        try:
                            attr = getattr(self, attr_name)
                            if hasattr(attr, 'apply') and callable(attr.apply):
                                attr.apply(fn)
                        except:
                            pass
                    return self
            
            class Linear(Module):
                def __init__(self, in_features, out_features, bias=True):
                    self.in_features = in_features
                    self.out_features = out_features
                    self.bias = bias
                    self.weight = MockTensor([out_features, in_features])
                    if bias:
                        self.bias = MockTensor([out_features])
                    else:
                        self.bias = None
            
            class Embedding(Module):
                def __init__(self, vocab_size, embed_dim):
                    self.vocab_size = vocab_size
                    self.embed_dim = embed_dim
                    self.weight = MockTensor([vocab_size, embed_dim])
            
            class LayerNorm(Module):
                def __init__(self, normalized_shape):
                    self.normalized_shape = normalized_shape
            
            class Dropout(Module):
                def __init__(self, p=0.5):
                    self.p = p
            
            class ModuleList(list):
                def __init__(self, modules):
                    super().__init__(modules)
            
            class functional:
                @staticmethod
                def mse_loss(input, target):
                    return MockTensor([])
                
                @staticmethod
                def softmax(input, dim):
                    return MockTensor(input.shape)
                
                @staticmethod
                def gelu(input):
                    return MockTensor(input.shape)
                
                @staticmethod
                def dropout(input, p=0.5, training=False):
                    return MockTensor(input.shape)
        
        class Size(tuple):
            def __getitem__(self, key):
                if isinstance(key, slice):
                    return list(self)[key]
                return super().__getitem__(key)
    
    # æ›¿æ¢torchå¯¼å…¥
    sys.modules['torch'] = MockTorch()
    sys.modules['torch.nn'] = MockTorch.nn
    sys.modules['torch.nn.functional'] = MockTorch.nn.functional
    
    import torch

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å‰å‘æ•°æ®æ•è·åŸºæœ¬åŠŸèƒ½...")
    
    try:
        from app.models.transformer.config import GPT2Config
        from app.models.transformer.model import GPT2Model
        from app.services.forward_capture import æ•°æ®æ•è·å®¹å™¨
        from app.schemas.forward_capture import æ•°æ®æ•è·é…ç½®
        
        print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰æ¨¡å—")
        
        # åˆ›å»ºæ ‡å‡†æ¨¡å‹é…ç½®
        config = GPT2Config(
            vocab_size=1000,
            context_size=256,
            n_layer=2,  # å‡å°‘å±‚æ•°ä»¥åŠ å¿«æµ‹è¯•
            n_head=8,
            n_embed=256,
            dropout=0.0,
            use_sparse_attention=False,
            use_moe=False,
        )
        
        print("âœ… æˆåŠŸåˆ›å»ºæ¨¡å‹é…ç½®")
        
        # åˆ›å»ºæ¨¡å‹
        model = GPT2Model(config)
        model.eval()
        
        print("âœ… æˆåŠŸåˆ›å»ºæ¨¡å‹")
        
        # åˆ›å»ºè¾“å…¥æ•°æ®
        input_ids = torch.randint(0, config.vocab_size, (2, 16))
        print(f"âœ… åˆ›å»ºè¾“å…¥æ•°æ®: {input_ids.shape}")
        
        # é…ç½®æ•°æ®æ•è·
        capture_config = æ•°æ®æ•è·é…ç½®(
            æ•è·åµŒå…¥æ•°æ®=True,
            æ•è·æ³¨æ„åŠ›æ•°æ®=True,
            æ•è·MoEæ•°æ®=True,
            æ•è·æœ€ç»ˆè¾“å‡º=True,
            æ•è·å¼ é‡å€¼=False,  # ä¸æ•è·å¼ é‡å€¼ä»¥èŠ‚çœå†…å­˜
        )
        
        print("âœ… æˆåŠŸåˆ›å»ºæ•è·é…ç½®")
        
        # æ‰§è¡Œå¸¦æ•°æ®æ•è·çš„å‰å‘ä¼ æ’­
        result = model.forward_with_capture(
            input_ids=input_ids,
            capture_config=capture_config
        )
        
        print("âœ… æˆåŠŸæ‰§è¡Œå‰å‘ä¼ æ’­å’Œæ•°æ®æ•è·")
        
        # éªŒè¯ç»“æœç»“æ„
        assert "logits" in result, "ç¼ºå°‘logits"
        assert "trajectory" in result, "ç¼ºå°‘trajectory"
        assert "capture_stats" in result, "ç¼ºå°‘capture_stats"
        
        print("âœ… ç»“æœç»“æ„éªŒè¯é€šè¿‡")
        
        # éªŒè¯è½¨è¿¹æ•°æ®
        trajectory = result["trajectory"]
        assert trajectory.æ‰¹æ¬¡å¤§å° == 2, f"æ‰¹æ¬¡å¤§å°é”™è¯¯: {trajectory.æ‰¹æ¬¡å¤§å°}"
        assert trajectory.åºåˆ—é•¿åº¦ == 16, f"åºåˆ—é•¿åº¦é”™è¯¯: {trajectory.åºåˆ—é•¿åº¦}"
        assert trajectory.åµŒå…¥æ•°æ® is not None, "åµŒå…¥æ•°æ®ä¸ºç©º"
        assert trajectory.æœ€ç»ˆè¾“å‡º is not None, "æœ€ç»ˆè¾“å‡ºä¸ºç©º"
        
        print("âœ… è½¨è¿¹æ•°æ®éªŒè¯é€šè¿‡")
        
        # éªŒè¯åµŒå…¥æ•°æ®
        åµŒå…¥ = trajectory.åµŒå…¥æ•°æ®
        assert åµŒå…¥.è¾“å…¥åºåˆ—é•¿åº¦ == 16, f"åµŒå…¥åºåˆ—é•¿åº¦é”™è¯¯: {åµŒå…¥.è¾“å…¥åºåˆ—é•¿åº¦}"
        assert åµŒå…¥.æ‰¹æ¬¡å¤§å° == 2, f"åµŒå…¥æ‰¹æ¬¡å¤§å°é”™è¯¯: {åµŒå…¥.æ‰¹æ¬¡å¤§å°}"
        assert åµŒå…¥.åµŒå…¥ç»´åº¦ == config.n_embed, f"åµŒå…¥ç»´åº¦é”™è¯¯: {åµŒå…¥.åµŒå…¥ç»´åº¦}"
        
        print("âœ… åµŒå…¥æ•°æ®éªŒè¯é€šè¿‡")
        
        # éªŒè¯æœ€ç»ˆè¾“å‡º
        è¾“å‡º = trajectory.æœ€ç»ˆè¾“å‡º
        assert è¾“å‡º.Logitså½¢çŠ¶ == [2, 16, config.vocab_size], f"Logitså½¢çŠ¶é”™è¯¯: {è¾“å‡º.Logitså½¢çŠ¶}"
        assert è¾“å‡º.è¯æ±‡è¡¨å¤§å° == config.vocab_size, f"è¯æ±‡è¡¨å¤§å°é”™è¯¯: {è¾“å‡º.è¯æ±‡è¡¨å¤§å°}"
        
        print("âœ… æœ€ç»ˆè¾“å‡ºéªŒè¯é€šè¿‡")
        
        # æµ‹è¯•JSONåºåˆ—åŒ–
        json_data = trajectory.model_dump_json()
        assert len(json_data) > 0, "JSONåºåˆ—åŒ–ç»“æœä¸ºç©º"
        
        print("âœ… JSONåºåˆ—åŒ–éªŒè¯é€šè¿‡")
        
        print("\nğŸ‰ æ‰€æœ‰åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_sparse_attention():
    """æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›æ•°æ®æ•è·...")
    
    try:
        from app.models.transformer.config import GPT2Config
        from app.models.transformer.model import GPT2Model
        from app.schemas.forward_capture import æ•°æ®æ•è·é…ç½®
        
        # åˆ›å»ºç¨€ç–æ³¨æ„åŠ›æ¨¡å‹é…ç½®
        config = GPT2Config(
            vocab_size=1000,
            context_size=256,
            n_layer=2,
            n_head=12,  # éœ€è¦èƒ½è¢«3æ•´é™¤
            n_embed=384,
            dropout=0.0,
            use_sparse_attention=True,
            use_moe=False,
        )
        
        model = GPT2Model(config)
        model.eval()
        
        input_ids = torch.randint(0, config.vocab_size, (2, 32))
        
        capture_config = æ•°æ®æ•è·é…ç½®(
            æ•è·åµŒå…¥æ•°æ®=True,
            æ•è·æ³¨æ„åŠ›æ•°æ®=True,
            æ•è·MoEæ•°æ®=False,
            æ•è·æœ€ç»ˆè¾“å‡º=True,
        )
        
        result = model.forward_with_capture(
            input_ids=input_ids,
            capture_config=capture_config
        )
        
        trajectory = result["trajectory"]
        
        # éªŒè¯ç¨€ç–æ³¨æ„åŠ›æ•°æ®
        assert len(trajectory.Transformerå±‚æ•°æ®) == config.n_layer
        
        for å±‚æ•°æ® in trajectory.Transformerå±‚æ•°æ®:
            if å±‚æ•°æ®.æ³¨æ„åŠ›æ•°æ®:
                æ³¨æ„åŠ› = å±‚æ•°æ®.æ³¨æ„åŠ›æ•°æ®
                assert æ³¨æ„åŠ›.å±€éƒ¨æ³¨æ„åŠ›å¤´æ•° > 0, "å±€éƒ¨æ³¨æ„åŠ›å¤´æ•°åº”å¤§äº0"
                assert æ³¨æ„åŠ›.å…¨å±€æ³¨æ„åŠ›å¤´æ•° > 0, "å…¨å±€æ³¨æ„åŠ›å¤´æ•°åº”å¤§äº0"
                assert æ³¨æ„åŠ›.æ³¨æ„åŠ›å¤´æ•°é‡ == config.n_head
                
                print(f"  ç¬¬{å±‚æ•°æ®.å±‚ç´¢å¼•}å±‚: {æ³¨æ„åŠ›.å±€éƒ¨æ³¨æ„åŠ›å¤´æ•°}å±€éƒ¨ + {æ³¨æ„åŠ›.å…¨å±€æ³¨æ„åŠ›å¤´æ•°}å…¨å±€å¤´")
        
        print("âœ… ç¨€ç–æ³¨æ„åŠ›æ•°æ®æ•è·æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ç¨€ç–æ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_moe():
    """æµ‹è¯•MoEåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•MoEæ•°æ®æ•è·...")
    
    try:
        from app.models.transformer.config import GPT2Config
        from app.models.transformer.model import GPT2Model
        from app.schemas.forward_capture import æ•°æ®æ•è·é…ç½®
        
        # åˆ›å»ºMoEæ¨¡å‹é…ç½®
        config = GPT2Config(
            vocab_size=1000,
            context_size=256,
            n_layer=2,
            n_head=8,
            n_embed=256,
            dropout=0.0,
            use_sparse_attention=False,
            use_moe=True,
            moe_num_experts=4,
            moe_top_k=2,
        )
        
        model = GPT2Model(config)
        model.eval()
        
        input_ids = torch.randint(0, config.vocab_size, (2, 20))
        
        capture_config = æ•°æ®æ•è·é…ç½®(
            æ•è·åµŒå…¥æ•°æ®=True,
            æ•è·æ³¨æ„åŠ›æ•°æ®=True,
            æ•è·MoEæ•°æ®=True,
            æ•è·æœ€ç»ˆè¾“å‡º=True,
        )
        
        result = model.forward_with_capture(
            input_ids=input_ids,
            capture_config=capture_config
        )
        
        trajectory = result["trajectory"]
        
        # éªŒè¯MoEæ•°æ®
        moe_layers_found = 0
        for å±‚æ•°æ® in trajectory.Transformerå±‚æ•°æ®:
            if å±‚æ•°æ®.MoEæ•°æ®:
                moe_layers_found += 1
                MoE = å±‚æ•°æ®.MoEæ•°æ®
                assert MoE.ä¸“å®¶æ€»æ•° == config.moe_num_experts
                assert MoE.TopKå€¼ == config.moe_top_k
                assert len(MoE.ä¸“å®¶ä¿¡æ¯åˆ—è¡¨) == config.moe_num_experts
                
                print(f"  ç¬¬{å±‚æ•°æ®.å±‚ç´¢å¼•}å±‚: {MoE.ä¸“å®¶æ€»æ•°}ä¸“å®¶, TopK={MoE.TopKå€¼}")
                print(f"    è´Ÿè½½å‡è¡¡æŸå¤±: {MoE.è´Ÿè½½å‡è¡¡æŸå¤±:.6f}")
        
        print(f"âœ… MoEæ•°æ®æ•è·æµ‹è¯•é€šè¿‡ï¼å‘ç°{moe_layers_found}ä¸ªMoEå±‚")
        return True
        
    except Exception as e:
        print(f"âŒ MoEæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å‰å‘æ•°æ®æ•è·ç³»ç»Ÿç®€å•æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        test_basic_functionality,
        test_sparse_attention,
        test_moe,
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å‰å‘æ•°æ®æ•è·ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1

if __name__ == "__main__":
    exit(main())