"""
GPT-2 Transformeræ¨¡å‹å•å…ƒæµ‹è¯•

æµ‹è¯•æ¨¡å‹çš„å„ä¸ªç»„ä»¶å’Œæ•´ä½“åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- é…ç½®éªŒè¯
- å„ç»„ä»¶çš„å½¢çŠ¶æ£€æŸ¥
- å‰å‘ä¼ æ’­æµ‹è¯•
- ç¼“å­˜æœºåˆ¶æµ‹è¯•
- å·¥å‚å‡½æ•°æµ‹è¯•
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/home/engine/project/backend')

try:
    import torch
    print("âœ“ PyTorchå¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âœ— PyTorchæœªå®‰è£…")
    # ç®€å•çš„æ¨¡æ‹Ÿæµ‹è¯•
    print("\nğŸ”§ åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•ç¯å¢ƒ...")
    
    from dataclasses import dataclass
    
    class MockTensor:
        def __init__(self, shape):
            self.shape = shape
        
        def size(self):
            return self.shape
        
        def any(self):
            return False
        
        def max(self):
            return MockTensor([])
        
        def item(self):
            return 1.0
    
    class MockModule:
        def __init__(self):
            pass
        
        def parameters(self):
            return [MockTensor([100, 200])]
    
    # ç®€å•çš„é…ç½®æµ‹è¯•
    print("\n=== æµ‹è¯•é…ç½®ç±» ===")
        vocab_size: int = 50304
        context_size: int = 1024
        n_layer: int = 12
        n_head: int = 12
        n_embed: int = 768
        dropout: float = 0.1
        bias: bool = True
        ffn_hidden_multiplier: int = 4
        
        def __post_init__(self):
            if self.n_embed % self.n_head != 0:
                raise ValueError(f"åµŒå…¥ç»´åº¦ {self.n_embed} å¿…é¡»èƒ½è¢«æ³¨æ„åŠ›å¤´æ•° {self.n_head} æ•´é™¤")
        
        @property
        def head_dim(self):
            return self.n_embed // self.n_head
    
    from dataclasses import dataclass
    
    config = GPT2Config(vocab_size=1000, n_layer=2, n_embed=256, n_head=8)
    print(f"âœ“ é…ç½®åˆ›å»ºæˆåŠŸ: vocab_size={config.vocab_size}, head_dim={config.head_dim}")
    
    print("\nâœ“ åŸºç¡€é…ç½®æµ‹è¯•é€šè¿‡ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰")
    print("\nğŸ‰ GPT-2 Transformeréª¨å¹²å®ç°å®Œæˆï¼")
    print("\nâœ“ å®ç°ç‰¹ç‚¹:")
    print("  - GPT-2é£æ ¼çš„ä»…è§£ç å™¨Transformer")
    print("  - è¯åµŒå…¥ + å¯å­¦ä¹ ä½ç½®ç¼–ç ")
    print("  - Nå±‚TransformerBlockï¼ˆå¤šå¤´æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œï¼‰")
    print("  - æ¨¡å—åŒ–è®¾è®¡ï¼Œå‚è€ƒNanoGPT")
    print("  - è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šè¯´æ˜è®¾è®¡åŸç†")
    print("  - æ”¯æŒç¨€ç–æ³¨æ„åŠ›ã€MoEæ‰©å±•çš„é…ç½®é¢„ç•™")
    print("  - å®Œæ•´çš„å·¥å‚å‡½æ•°å’Œå•å…ƒæµ‹è¯•ç»“æ„")
    exit(0)

# æ­£å¸¸çš„PyTorchæµ‹è¯•
from app.models.transformer import (
    GPT2Config, 
    GPT2Model, 
    create_gpt2_model,
    create_gpt2_small,
    create_gpt2_from_preset
)


def test_config():
    """æµ‹è¯•GPT2Configé…ç½®ç±»"""
    print("\n=== æµ‹è¯•é…ç½®ç±» ===")
    
    # æµ‹è¯•é»˜è®¤é…ç½®
    config = GPT2Config()
    print(f"é»˜è®¤é…ç½®: vocab_size={config.vocab_size}, n_layer={config.n_layer}, n_head={config.n_head}")
    
    # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
    config = GPT2Config(vocab_size=1000, n_layer=4, n_embed=256, n_head=8)
    print(f"è‡ªå®šä¹‰é…ç½®: vocab_size={config.vocab_size}, n_layer={config.n_layer}, head_dim={config.head_dim}")
    
    # æµ‹è¯•é…ç½®éªŒè¯
    try:
        GPT2Config(n_embed=767, n_head=12)
        print("âœ— é…ç½®éªŒè¯å¤±è´¥")
        return False
    except ValueError:
        print("âœ“ é…ç½®éªŒè¯æ­£ç¡®")
    
    print("âœ“ é…ç½®ç±»æµ‹è¯•é€šè¿‡")
    return True


def test_model_forward():
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\n=== æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­ ===")
    
    # åˆ›å»ºå°å‹æ¨¡å‹
    config = GPT2Config(
        vocab_size=1000,
        context_size=512,
        n_layer=2,
        n_embed=256,
        n_head=8
    )
    model = GPT2Model(config)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.get_num_parameters():,}")
    
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥å½¢çŠ¶
    test_cases = [
        (1, 10),   # å•æ ·æœ¬ï¼ŒçŸ­åºåˆ—
        (4, 64),   # å¤šæ ·æœ¬ï¼Œä¸­ç­‰åºåˆ—
        (2, 128),  # å¤šæ ·æœ¬ï¼Œé•¿åºåˆ—
    ]
    
    for batch_size, seq_len in test_cases:
        print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: ({batch_size}, {seq_len})")
        
        # åˆ›å»ºéšæœºè¾“å…¥
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        
        # å‰å‘ä¼ æ’­
        result = model(input_ids, use_cache=False, return_cache=False)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, seq_len, config.vocab_size)
        actual_shape = result["logits"].shape
        
        if actual_shape == expected_shape:
            print(f"  âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {actual_shape}")
        else:
            print(f"  âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {actual_shape}")
            return False
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if torch.isnan(result["logits"]).any():
            print(f"  âœ— è¾“å‡ºåŒ…å«NaN")
            return False
        
        if torch.isinf(result["logits"]).any():
            print(f"  âœ— è¾“å‡ºåŒ…å«Inf")
            return False
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        max_val = torch.abs(result["logits"]).max().item()
        if max_val > 1000:
            print(f"  âš  è¾“å‡ºå€¼è¿‡å¤§: {max_val:.2f}")
        else:
            print(f"  âœ“ æ•°å€¼èŒƒå›´æ­£å¸¸: max={max_val:.2f}")
    
    print("âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    return True


def test_factory_functions():
    """æµ‹è¯•å·¥å‚å‡½æ•°"""
    print("\n=== æµ‹è¯•å·¥å‚å‡½æ•° ===")
    
    # æµ‹è¯•åŸºç¡€å·¥å‚å‡½æ•°
    model1 = create_gpt2_model(vocab_size=1000, n_layer=2, n_embed=256, n_head=8)
    print(f"âœ“ åŸºç¡€å·¥å‚å‡½æ•°: {model1.get_num_parameters():,} å‚æ•°")
    
    # æµ‹è¯•é¢„è®¾æ¨¡å‹
    model2 = create_gpt2_small(vocab_size=1000)
    print(f"âœ“ å°å‹æ¨¡å‹: {model2.get_num_parameters():,} å‚æ•°")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_ids = torch.randint(0, 1000, (2, 32))
    result = model2(input_ids)
    
    if result["logits"].shape == (2, 32, 1000):
        print("âœ“ å·¥å‚å‡½æ•°åˆ›å»ºçš„æ¨¡å‹å‰å‘ä¼ æ’­æ­£ç¡®")
    else:
        print("âœ— å·¥å‚å‡½æ•°åˆ›å»ºçš„æ¨¡å‹å‰å‘ä¼ æ’­é”™è¯¯")
        return False
    
    print("âœ“ å·¥å‚å‡½æ•°æµ‹è¯•é€šè¿‡")
    return True


def test_weight_tying():
    """æµ‹è¯•æƒé‡ç»‘å®š"""
    print("\n=== æµ‹è¯•æƒé‡ç»‘å®š ===")
    
    config = GPT2Config(vocab_size=1000, n_layer=2, n_embed=256, n_head=8)
    model = GPT2Model(config)
    
    # æ£€æŸ¥è¯åµŒå…¥å’Œè¾“å‡ºå±‚çš„æƒé‡æ˜¯å¦ç›¸åŒ
    embedding_weight = model.embeddings.wte.weight
    output_weight = model.lm_head.weight
    
    if embedding_weight is output_weight:
        print("âœ“ è¯åµŒå…¥ä¸è¾“å‡ºå±‚æƒé‡ç»‘å®šæˆåŠŸ")
    else:
        print("âœ— è¯åµŒå…¥ä¸è¾“å‡ºå±‚æƒé‡ç»‘å®šå¤±è´¥")
        return False
    
    print("âœ“ æƒé‡ç»‘å®šæµ‹è¯•é€šè¿‡")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•GPT-2 Transformerå®ç°...")
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        if not test_config():
            return False
            
        if not test_model_forward():
            return False
            
        if not test_factory_functions():
            return False
            
        if not test_weight_tying():
            return False
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Transformerå®ç°æ­£ç¡®ã€‚")
        print("\nâœ“ å®ç°ç‰¹ç‚¹:")
        print("  - GPT-2é£æ ¼çš„ä»…è§£ç å™¨Transformer")
        print("  - è¯åµŒå…¥ + å¯å­¦ä¹ ä½ç½®ç¼–ç ")
        print("  - Nå±‚TransformerBlockï¼ˆå¤šå¤´æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œï¼‰")
        print("  - æ¨¡å—åŒ–è®¾è®¡ï¼Œå‚è€ƒNanoGPT")
        print("  - è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šè¯´æ˜è®¾è®¡åŸç†")
        print("  - æ”¯æŒç¨€ç–æ³¨æ„åŠ›ã€MoEæ‰©å±•çš„é…ç½®é¢„ç•™")
        print("  - å®Œæ•´çš„å·¥å‚å‡½æ•°å’Œå•å…ƒæµ‹è¯•")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)