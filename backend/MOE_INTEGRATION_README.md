# MoEå±‚é›†æˆå®ç°

æœ¬æ–‡æ¡£æè¿°äº†Mixture of Experts (MoE) å±‚åœ¨Transformeræ¨¡å‹ä¸­çš„é›†æˆå®ç°ã€‚

## ğŸ“‹ å®ç°æ¦‚è¿°

### æ ¸å¿ƒåŠŸèƒ½
- âœ… å®ç°ç‹¬ç«‹çš„ `MoELayer`ï¼ŒåŒ…å« gating ç½‘ç»œã€Top-k è·¯ç”±å’Œå¤šä¸ªå¹¶è¡Œä¸“å®¶
- âœ… å°† TransformerBlock ä¸­çš„ FFN æ›¿æ¢ä¸º MoE å±‚
- âœ… æä¾›ä¸°å¯Œçš„é…ç½®é€‰é¡¹æ§åˆ¶ä¸“å®¶æ•°é‡ã€æ¿€æ´»å‡½æ•°ã€dropout
- âœ… æ•è·å®Œæ•´çš„ä¸­é—´æ•°æ®ï¼Œç¡®ä¿å¯åºåˆ—åŒ–ä¸åç«¯è¿”å›
- âœ… æ·»åŠ å•å…ƒæµ‹è¯•éªŒè¯ Top-k è·¯ç”±æ­£ç¡®ã€æƒé‡å½’ä¸€åŒ–ã€æ¢¯åº¦å¯åå‘ä¼ æ’­

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### 1. MoELayer (ä¸»è¦å±‚)
```python
class MoELayer(nn.Module):
    """Mixture of Experts å±‚"""
    
    def __init__(self, config, num_experts=8, top_k=2):
        # åˆ›å»ºä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([...])
        # åˆ›å»ºgatingç½‘ç»œ
        self.gating_network = GatingNetwork(...)
    
    def forward(self, x, return_intermediate=False):
        # 1. è®¡ç®—gatingåˆ†æ•°
        # 2. Top-kè·¯ç”±
        # 3. ä¸“å®¶å¤„ç†
        # 4. åŠ æƒç»„åˆ
        # 5. è¿”å›è¾“å‡ºå’Œä¸­é—´æ•°æ®
```

### 2. MoEExpert (ä¸“å®¶ç½‘ç»œ)
```python
class MoEExpert(nn.Module):
    """MoEä¸“å®¶ç½‘ç»œ"""
    
    def __init__(self, config):
        # æ ‡å‡†FFNæ¶æ„ï¼šLinear -> Activation -> Linear -> Dropout
        self.c_fc = nn.Linear(...)
        self.c_proj = nn.Linear(...)
```

### 3. GatingNetwork (é—¨æ§ç½‘ç»œ)
```python
class GatingNetwork(nn.Module):
    """Gatingç½‘ç»œ"""
    
    def __init__(self, n_embed, num_experts):
        # ç®€å•çº¿æ€§å±‚ + softmax
        self.gate = nn.Linear(n_embed, num_experts)
```

## âš™ï¸ é…ç½®é€‰é¡¹

### æ–°å¢é…ç½®å‚æ•°
```python
@dataclass
class GPT2Config:
    # MoEé…ç½®
    use_moe: bool = False              # MoEå¼€å…³
    moe_num_experts: int = 8           # ä¸“å®¶æ•°é‡
    moe_top_k: int = 2                 # Top-kè·¯ç”±
    moe_activation: str = "gelu"       # æ¿€æ´»å‡½æ•°
    moe_dropout: Optional[float] = None # ä¸“ç”¨dropout
```

### æ”¯æŒçš„æ¿€æ´»å‡½æ•°
- `gelu`: GELUæ¿€æ´»å‡½æ•° (é»˜è®¤)
- `relu`: ReLUæ¿€æ´»å‡½æ•°
- `swish`: Swish/SiLUæ¿€æ´»å‡½æ•°
- `tanh`: Tanhæ¿€æ´»å‡½æ•°

## ğŸ”„ é›†æˆæ–¹å¼

### TransformerBlockä¿®æ”¹
```python
class TransformerBlock(nn.Module):
    def __init__(self, config):
        # æ ¹æ®é…ç½®é€‰æ‹©FFNæˆ–MoE
        if config.use_moe:
            self.mlp = MoELayer(config, 
                              num_experts=config.moe_num_experts,
                              top_k=config.moe_top_k)
        else:
            self.mlp = FeedForward(config)
    
    def forward(self, x, return_intermediate=False):
        # å¤„ç†MoEä¸­é—´æ•°æ®
        if isinstance(self.mlp, MoELayer):
            mlp_output, moe_intermediate = self.mlp(x, return_intermediate)
        else:
            mlp_output = self.mlp(x)
            moe_intermediate = None
```

## ğŸ“Š ä¸­é—´æ•°æ®æ•è·

MoEå±‚å¯ä»¥æ•è·ä»¥ä¸‹ä¸­é—´æ•°æ®ï¼š

```python
intermediate = {
    'gate_scores': torch.Tensor,      # æ‰€æœ‰ä¸“å®¶çš„é—¨æ§åˆ†æ•°
    'top_k_scores': torch.Tensor,     # Top-kä¸“å®¶åˆ†æ•°
    'top_k_indices': torch.Tensor,     # Top-kä¸“å®¶ç´¢å¼•
    'expert_outputs': List[Dict],      # å„ä¸“å®¶è¾“å‡ºè¯¦æƒ…
    'final_output': torch.Tensor,      # æœ€ç»ˆåŠ æƒè¾“å‡º
    'load_balance_loss': torch.Tensor   # è´Ÿè½½å‡è¡¡æŸå¤±
}
```

### ä½¿ç”¨ç¤ºä¾‹
```python
# åˆ›å»ºMoEé…ç½®
config = GPT2Config(
    n_embed=768,
    n_head=12,
    use_moe=True,
    moe_num_experts=8,
    moe_top_k=2
)

# åˆ›å»ºTransformerBlock
block = TransformerBlock(config)

# å‰å‘ä¼ æ’­å¹¶è·å–ä¸­é—´æ•°æ®
output, cache, intermediate = block(
    x, 
    use_cache=False, 
    return_intermediate=True
)

# è®¿é—®MoEä¸­é—´æ•°æ®
moe_data = intermediate['moe']
gate_scores = moe_data['gate_scores']
top_k_indices = moe_data['top_k_indices']
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### å•å…ƒæµ‹è¯•è¦†ç›–
- âœ… MoEä¸“å®¶ç½‘ç»œå‰å‘ä¼ æ’­
- âœ… Gatingç½‘ç»œæ¦‚ç‡å½’ä¸€åŒ–
- âœ… Top-kè·¯ç”±æ­£ç¡®æ€§
- âœ… æƒé‡å½’ä¸€åŒ–éªŒè¯
- âœ… æ¢¯åº¦åå‘ä¼ æ’­
- âœ… è´Ÿè½½å‡è¡¡æŸå¤±è®¡ç®—
- âœ… ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
- âœ… ä¸åŒé…ç½®ç»„åˆ
- âœ… TransformerBlocké›†æˆ
- âœ… é…ç½®éªŒè¯

### è¿è¡Œæµ‹è¯•
```bash
# åŸºæœ¬åŠŸèƒ½æµ‹è¯•
python3 test_moe_basic.py

# å®Œæ•´å•å…ƒæµ‹è¯• (éœ€è¦torch)
python3 test_moe_unit.py

# é›†æˆæ¼”ç¤º
python3 demo_moe_integration.py
```

## ğŸ”§ æ ¸å¿ƒç®—æ³•

### Top-kè·¯ç”±ç®—æ³•
```python
# 1. è®¡ç®—gatingåˆ†æ•°
gate_scores = self.gating_network(x)  # (B, L, E)

# 2. é€‰æ‹©top-kä¸“å®¶
top_k_scores, top_k_indices = torch.topk(
    gate_scores, self.top_k, dim=-1, sorted=True
)

# 3. å½’ä¸€åŒ–top-kåˆ†æ•°
top_k_scores = top_k_scores / (
    top_k_scores.sum(dim=-1, keepdim=True) + 1e-8
)

# 4. ä¸“å®¶å¤„ç†å’ŒåŠ æƒç»„åˆ
for expert_idx, expert in enumerate(self.experts):
    expert_mask = (top_k_indices == expert_idx).any(dim=-1)
    if expert_mask.any():
        # å¤„ç†å¯¹åº”tokenå¹¶åŠ æƒ
        expert_input = x[expert_mask]
        expert_output = expert(expert_input)
        weighted_output = expert_output * expert_weights
        output[expert_mask] += weighted_output
```

### è´Ÿè½½å‡è¡¡æŸå¤±
```python
def compute_load_balance_loss(self, gate_scores):
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡ä½¿ç”¨é¢‘ç‡
    expert_usage = gate_scores.mean(dim=(0, 1))
    
    # ç†æƒ³ä½¿ç”¨é¢‘ç‡
    ideal_usage = 1.0 / self.num_experts
    
    # æ–¹å·®ä½œä¸ºè´Ÿè½½å‡è¡¡æŸå¤±
    load_balance_loss = torch.var(expert_usage - ideal_usage)
    
    return self.load_balance_loss_coef * load_balance_loss
```

## ğŸ“ˆ æ€§èƒ½ç‰¹æ€§

### è®¡ç®—å¤æ‚åº¦
- **Gatingç½‘ç»œ**: O(B Ã— L Ã— E Ã— H) - çº¿æ€§å¤æ‚åº¦
- **ä¸“å®¶å¤„ç†**: O(k Ã— B Ã— L Ã— HÂ²) - åªè®¡ç®—é€‰ä¸­çš„kä¸ªä¸“å®¶
- **æ€»ä½“å¤æ‚åº¦**: ç›¸æ¯”æ ‡å‡†FFNï¼Œå½“k << Eæ—¶æœ‰æ˜¾è‘—èŠ‚çœ

### å†…å­˜ä½¿ç”¨
- ä¸“å®¶å‚æ•°: E Ã— H Ã— 4H (æ ‡å‡†FFNçš„Eå€)
- æ¿€æ´»å†…å­˜: ä¸æ ‡å‡†FFNç›¸å½“ (åªå­˜å‚¨é€‰ä¸­ä¸“å®¶çš„æ¿€æ´»)
- ä¸­é—´æ•°æ®: å¯é€‰å­˜å‚¨ï¼Œç”¨äºè°ƒè¯•å’Œåˆ†æ

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### é€‚ç”¨åœºæ™¯
- **å¤§è§„æ¨¡æ¨¡å‹**: å½“æ¨¡å‹å‚æ•°é‡å¾ˆå¤§æ—¶ï¼ŒMoEå¯ä»¥æ˜¾è‘—å¢åŠ è®¡ç®—èƒ½åŠ›
- **å¤šæ ·åŒ–ä»»åŠ¡**: ä¸åŒtokenå¯èƒ½éœ€è¦ä¸åŒçš„ä¸“ä¸šçŸ¥è¯†
- **æ¨ç†åŠ é€Ÿ**: åœ¨æ¨ç†æ—¶å¯ä»¥åªä½¿ç”¨éƒ¨åˆ†ä¸“å®¶

### é…ç½®å»ºè®®
- **å°æ¨¡å‹** (< 1Bå‚æ•°): ä½¿ç”¨2-4ä¸ªä¸“å®¶ï¼Œtop_k=1
- **ä¸­ç­‰æ¨¡å‹** (1B-10Bå‚æ•°): ä½¿ç”¨8-16ä¸ªä¸“å®¶ï¼Œtop_k=2
- **å¤§æ¨¡å‹** (> 10Bå‚æ•°): ä½¿ç”¨16-64ä¸ªä¸“å®¶ï¼Œtop_k=2-4

## ğŸš€ æœªæ¥æ‰©å±•

### å¯èƒ½çš„æ”¹è¿›
1. **åŠ¨æ€ä¸“å®¶æ•°é‡**: æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´ä¸“å®¶æ•°é‡
2. **ä¸“å®¶ä¸“ä¸šåŒ–**: è®©ä¸åŒä¸“å®¶ä¸“é—¨å¤„ç†ç‰¹å®šç±»å‹çš„token
3. **å±‚çº§è·¯ç”±**: å®ç°å¤šçº§ä¸“å®¶è·¯ç”±æœºåˆ¶
4. **ä¸“å®¶å‰ªæ**: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç§»é™¤ä¸é‡è¦çš„ä¸“å®¶
5. **çŸ¥è¯†è’¸é¦**: å°†MoEæ¨¡å‹è’¸é¦ä¸ºç¨ å¯†æ¨¡å‹

### é›†æˆå…¶ä»–æŠ€æœ¯
- ä¸ç¨€ç–æ³¨æ„åŠ›ç»“åˆ
- æ”¯æŒä¸“å®¶é—´çš„ä¿¡æ¯å…±äº«
- å®ç°ä¸“å®¶çš„å¢é‡å­¦ä¹ 

## ğŸ“ æ€»ç»“

æœ¬å®ç°æä¾›äº†ä¸€ä¸ªå®Œæ•´ã€çµæ´»ä¸”é«˜æ€§èƒ½çš„MoEå±‚é›†æˆæ–¹æ¡ˆï¼š

- **å®Œæ•´æ€§**: åŒ…å«æ‰€æœ‰å¿…è¦çš„ç»„ä»¶å’ŒåŠŸèƒ½
- **çµæ´»æ€§**: ä¸°å¯Œçš„é…ç½®é€‰é¡¹å’Œå‚æ•°
- **å¯æ‰©å±•æ€§**: æ˜“äºæ‰©å±•å’Œä¿®æ”¹
- **å¯è§‚æµ‹æ€§**: å®Œæ•´çš„ä¸­é—´æ•°æ®æ•è·
- **å¯é æ€§**: å…¨é¢çš„æµ‹è¯•éªŒè¯å’Œé”™è¯¯å¤„ç†

è¯¥å®ç°å¯ä»¥ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒä¸­çš„å¤§è§„æ¨¡Transformeræ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚