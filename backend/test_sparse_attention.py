"""
ç¨€ç–æ³¨æ„åŠ›æ¨¡å—å•å…ƒæµ‹è¯•

æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›çš„å„ä¸ªæ–¹é¢ï¼š
- é…ç½®éªŒè¯
- åˆ†ç»„å¤´æ³¨æ„åŠ›æœºåˆ¶
- ç¨€ç–æ©ç ç”Ÿæˆ
- æ•°å€¼ç¨³å®šæ€§
- ä¸­é—´å¼ é‡è¿”å›
- ä¸æ ‡å‡†æ³¨æ„åŠ›çš„å…¼å®¹æ€§
"""

import sys
import os
import math

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/home/engine/project/backend')

try:
    import torch
    import torch.nn.functional as F
    print("âœ“ PyTorchå¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âœ— PyTorchæœªå®‰è£…")
    exit(1)

from app.models.transformer import GPT2Config
from app.models.transformer.sparse_attention import SparseAttention, SparseAttentionConfig
from app.models.transformer.block import TransformerBlock


def test_sparse_attention_config():
    """æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›é…ç½®"""
    print("\n=== æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›é…ç½® ===")
    
    # æµ‹è¯•é»˜è®¤é…ç½®
    config = SparseAttentionConfig()
    print(f"é»˜è®¤é…ç½®: local_heads={config.local_heads}, global_heads={config.global_heads}")
    print(f"çª—å£å¤§å°: {config.window_size}, è‡ªé€‚åº”çª—å£: {config.adaptive_window}")
    
    # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
    custom_config = SparseAttentionConfig(
        local_heads=4,
        global_heads=2,
        window_size=64,
        adaptive_window=False
    )
    print(f"è‡ªå®šä¹‰é…ç½®: local_heads={custom_config.local_heads}, global_heads={custom_config.global_heads}")
    
    print("âœ“ ç¨€ç–æ³¨æ„åŠ›é…ç½®æµ‹è¯•é€šè¿‡")
    return True


def test_sparse_attention_initialization():
    """æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›åˆå§‹åŒ–"""
    print("\n=== æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›åˆå§‹åŒ– ===")
    
    # åˆ›å»ºåŸºç¡€é…ç½® - ç¡®ä¿n_embedèƒ½è¢«n_headæ•´é™¤
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=512,
        n_layer=2,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6      # ä¾¿äºæµ‹è¯•åˆ†ç»„
    )
    
    # æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›åˆå§‹åŒ–
    sparse_config = SparseAttentionConfig(
        local_heads=4,
        global_heads=2
    )
    
    sparse_attn = SparseAttention(gpt_config, sparse_config)
    print(f"ç¨€ç–æ³¨æ„åŠ›åˆå§‹åŒ–æˆåŠŸ: n_head={sparse_attn.n_head}, head_dim={sparse_attn.head_dim}")
    print(f"å±€éƒ¨å¤´ç´¢å¼•: {sparse_attn.local_head_indices}")
    print(f"å…¨å±€å¤´ç´¢å¼•: {sparse_attn.global_head_indices}")
    
    # æµ‹è¯•é…ç½®éªŒè¯
    try:
        wrong_config = SparseAttentionConfig(local_heads=3, global_heads=4)  # æ€»å…±7ä¸ªå¤´ï¼Œä½†é…ç½®æ˜¯6ä¸ª
        SparseAttention(gpt_config, wrong_config)
        print("âœ— é…ç½®éªŒè¯å¤±è´¥")
        return False
    except ValueError:
        print("âœ“ é…ç½®éªŒè¯æ­£ç¡®")
    
    print("âœ“ ç¨€ç–æ³¨æ„åŠ›åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
    return True


def test_dynamic_window_size():
    """æµ‹è¯•åŠ¨æ€çª—å£å¤§å°è®¡ç®—"""
    print("\n=== æµ‹è¯•åŠ¨æ€çª—å£å¤§å°è®¡ç®— ===")
    
    gpt_config = GPT2Config(n_head=6, n_embed=240)  # 240èƒ½è¢«6æ•´é™¤
    sparse_config = SparseAttentionConfig(
        local_heads=4,  # 4ä¸ªå±€éƒ¨å¤´
        global_heads=2, # 2ä¸ªå…¨å±€å¤´ï¼Œæ€»å…±6ä¸ªå¤´
        window_size=128,
        adaptive_window=True,
        min_window_size=32,
        max_window_size=512
    )
    sparse_attn = SparseAttention(gpt_config, sparse_config)
    
    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
    test_seq_lengths = [64, 128, 256, 512, 1024]
    
    for seq_len in test_seq_lengths:
        window_size = sparse_attn._compute_dynamic_window_size(seq_len)
        print(f"åºåˆ—é•¿åº¦ {seq_len:4d} -> çª—å£å¤§å° {window_size:4d}")
        
        # éªŒè¯çª—å£å¤§å°åœ¨åˆç†èŒƒå›´å†…
        if not (sparse_config.min_window_size <= window_size <= sparse_config.max_window_size):
            print(f"âœ— çª—å£å¤§å°è¶…å‡ºèŒƒå›´: {window_size}")
            return False
    
    print("âœ“ åŠ¨æ€çª—å£å¤§å°è®¡ç®—æµ‹è¯•é€šè¿‡")
    return True


def test_mask_generation():
    """æµ‹è¯•æ©ç ç”Ÿæˆ"""
    print("\n=== æµ‹è¯•æ©ç ç”Ÿæˆ ===")
    
    device = torch.device('cpu')
    seq_len = 10
    window_size = 3
    
    gpt_config = GPT2Config(n_head=6, n_embed=240)  # 240èƒ½è¢«6æ•´é™¤
    sparse_config = SparseAttentionConfig(
        local_heads=4,  # 4ä¸ªå±€éƒ¨å¤´
        global_heads=2, # 2ä¸ªå…¨å±€å¤´
        window_size=window_size
    )
    sparse_attn = SparseAttention(gpt_config, sparse_config)
    
    # æµ‹è¯•æœ¬åœ°æ©ç 
    local_mask = sparse_attn._generate_local_mask(seq_len, window_size, device)
    print(f"æœ¬åœ°æ©ç å½¢çŠ¶: {local_mask.shape}")
    print("æœ¬åœ°æ©ç ç¤ºä¾‹ (å‰5è¡Œå‰5åˆ—):")
    print(local_mask[:5, :5])
    
    # éªŒè¯æœ¬åœ°æ©ç ç‰¹æ€§
    for i in range(seq_len):
        # æ£€æŸ¥çª—å£èŒƒå›´ï¼ˆè€ƒè™‘å› æœæ©ç ï¼‰
        half_window = window_size // 2
        start = max(0, i - half_window)
        end = min(i + 1, i + half_window + 1)  # é™åˆ¶åˆ°i+1å› ä¸ºå› æœæ©ç 
        
        # çª—å£å†…åº”è¯¥ä¸º0ï¼ˆè€ƒè™‘å› æœæ€§ï¼‰
        for j in range(start, end):
            if local_mask[i, j] != 0:
                print(f"âœ— ä½ç½®{i}çš„çª—å£æ©ç é”™è¯¯: ä½ç½®{j}åº”è¯¥ä¸º0")
                return False
        
        # çª—å£å¤–ä¸”åœ¨å½“å‰ä½ç½®ä¹‹å‰åº”è¯¥ä¸ºmask_value
        for j in range(end, i + 1):
            if j < seq_len and local_mask[i, j] != sparse_config.mask_value:
                print(f"âœ— ä½ç½®{i}çš„çª—å£å¤–æ©ç é”™è¯¯: ä½ç½®{j}åº”è¯¥ä¸ºmask_value")
                return False
        
        # å› æœæ©ç ï¼šå½“å‰ä½ç½®ä¹‹ååº”è¯¥ä¸ºmask_value
        if i + 1 < seq_len and not torch.all(local_mask[i, i+1:] == sparse_config.mask_value):
            print(f"âœ— ä½ç½®{i}çš„å› æœæ©ç é”™è¯¯")
            return False
    
    # æµ‹è¯•å…¨å±€æ©ç 
    num_global_tokens = 2
    global_mask = sparse_attn._generate_global_mask(seq_len, num_global_tokens, device)
    print(f"\nå…¨å±€æ©ç å½¢çŠ¶: {global_mask.shape}")
    print("å…¨å±€æ©ç ç¤ºä¾‹ (å‰5è¡Œå‰5åˆ—):")
    print(global_mask[:5, :5])
    
    print("âœ“ æ©ç ç”Ÿæˆæµ‹è¯•é€šè¿‡")
    return True


def test_sparse_attention_forward():
    """æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›å‰å‘ä¼ æ’­"""
    print("\n=== æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›å‰å‘ä¼ æ’­ ===")
    
    # åˆ›å»ºé…ç½®
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=512,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6
    )
    
    sparse_config = SparseAttentionConfig(
        local_heads=4,
        global_heads=2,
        window_size=64,
        adaptive_window=False
    )
    
    sparse_attn = SparseAttention(gpt_config, sparse_config)
    
    # æµ‹è¯•ä¸åŒè¾“å…¥å½¢çŠ¶
    test_cases = [
        (1, 32),   # å•æ ·æœ¬ï¼ŒçŸ­åºåˆ—
        (2, 64),   # å¤šæ ·æœ¬ï¼Œä¸­ç­‰åºåˆ—
        (1, 128),  # å•æ ·æœ¬ï¼Œé•¿åºåˆ—
    ]
    
    for batch_size, seq_len in test_cases:
        print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: ({batch_size}, {seq_len})")
        
        # åˆ›å»ºéšæœºè¾“å…¥
        x = torch.randn(batch_size, seq_len, gpt_config.n_embed)
        
        # å‰å‘ä¼ æ’­ï¼ˆä¸è¿”å›ä¸­é—´å¼ é‡ï¼‰
        output, cache, _ = sparse_attn(x, use_cache=False, return_intermediate=False)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, seq_len, gpt_config.n_embed)
        if output.shape != expected_shape:
            print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {output.shape}")
            return False
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if torch.isnan(output).any():
            print(f"âœ— è¾“å‡ºåŒ…å«NaN")
            return False
        
        if torch.isinf(output).any():
            print(f"âœ— è¾“å‡ºåŒ…å«Inf")
            return False
        
        # å‰å‘ä¼ æ’­ï¼ˆè¿”å›ä¸­é—´å¼ é‡ï¼‰
        output, cache, intermediate = sparse_attn(x, use_cache=False, return_intermediate=True)
        
        # éªŒè¯ä¸­é—´å¼ é‡
        required_keys = ['qkv', 'q', 'k', 'v', 'local_mask', 'global_mask', 
                        'local_attn_scores', 'global_attn_weights', 'final_output']
        
        for key in required_keys:
            if key not in intermediate:
                print(f"âœ— ç¼ºå°‘ä¸­é—´å¼ é‡: {key}")
                return False
        
        print(f"  âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape}")
        print(f"  âœ“ ä¸­é—´å¼ é‡å®Œæ•´: {len(intermediate)} ä¸ª")
    
    print("âœ“ ç¨€ç–æ³¨æ„åŠ›å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    return True


def test_sparsity_characteristics():
    """æµ‹è¯•ç¨€ç–ç‰¹æ€§"""
    print("\n=== æµ‹è¯•ç¨€ç–ç‰¹æ€§ ===")
    
    # åˆ›å»ºé…ç½®
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=512,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6
    )
    
    sparse_config = SparseAttentionConfig(
        local_heads=4,
        global_heads=2,
        window_size=32,
        adaptive_window=False
    )
    
    sparse_attn = SparseAttention(gpt_config, sparse_config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size, seq_len = 1, 64
    x = torch.randn(batch_size, seq_len, gpt_config.n_embed)
    
    # è·å–ä¸­é—´å¼ é‡
    _, _, intermediate = sparse_attn(x, use_cache=False, return_intermediate=True)
    
    # æ£€æŸ¥æœ¬åœ°æ³¨æ„åŠ›æƒé‡çš„ç¨€ç–æ€§
    local_weights = intermediate['local_attn_scores']  # (batch_size, local_heads, seq_len, seq_len)
    
    # è®¡ç®—éé›¶å…ƒç´ æ¯”ä¾‹
    total_elements = local_weights.numel()
    zero_elements = (local_weights == sparse_config.mask_value).sum().item()
    sparsity_ratio = zero_elements / total_elements
    
    print(f"æœ¬åœ°æ³¨æ„åŠ›ç¨€ç–æ€§: {sparsity_ratio:.2%} (é›¶å…ƒç´ æ¯”ä¾‹)")
    
    # éªŒè¯ç¨€ç–æ€§åœ¨åˆç†èŒƒå›´å†…
    expected_sparsity = 1 - (sparse_config.window_size / seq_len)
    if abs(sparsity_ratio - expected_sparsity) > 0.2:  # å…è®¸20%çš„è¯¯å·®
        print(f"âš  ç¨€ç–æ€§å¯èƒ½ä¸ç¬¦åˆé¢„æœŸ: æœŸæœ›çº¦ {expected_sparsity:.2%}, å®é™… {sparsity_ratio:.2%}")
    else:
        print(f"âœ“ ç¨€ç–æ€§ç¬¦åˆé¢„æœŸ")
    
    # æ£€æŸ¥å…¨å±€æ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒ
    global_weights = intermediate['global_attn_weights']  # (batch_size, global_heads, seq_len, seq_len)
    
    # å…¨å±€æ³¨æ„åŠ›åº”è¯¥æœ‰æ›´å¤šçš„éé›¶å…ƒç´ 
    global_total = global_weights.numel()
    global_nonzero = (global_weights > 0).sum().item()
    global_density = global_nonzero / global_total
    
    print(f"å…¨å±€æ³¨æ„åŠ›å¯†åº¦: {global_density:.2%}")
    
    print("âœ“ ç¨€ç–ç‰¹æ€§æµ‹è¯•é€šè¿‡")
    return True


def test_numerical_stability():
    """æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
    print("\n=== æµ‹è¯•æ•°å€¼ç¨³å®šæ€§ ===")
    
    # åˆ›å»ºé…ç½®
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=512,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6
    )
    
    sparse_config = SparseAttentionConfig(
        local_heads=4,
        global_heads=2,
        window_size=32,
        mask_value=-1e9  # ä½¿ç”¨è¾ƒå¤§çš„è´Ÿå€¼
    )
    
    sparse_attn = SparseAttention(gpt_config, sparse_config)
    
    # æµ‹è¯•æç«¯æƒ…å†µ
    test_cases = [
        ("å¤§å€¼è¾“å…¥", torch.randn(1, 32, gpt_config.n_embed) * 100),
        ("å°å€¼è¾“å…¥", torch.randn(1, 32, gpt_config.n_embed) * 0.01),
        ("é›¶è¾“å…¥", torch.zeros(1, 32, gpt_config.n_embed)),
        ("é•¿åºåˆ—", torch.randn(1, 256, gpt_config.n_embed)),
    ]
    
    for case_name, x in test_cases:
        print(f"æµ‹è¯• {case_name}:")
        
        try:
            output, cache, intermediate = sparse_attn(x, use_cache=False, return_intermediate=True)
            
            # æ£€æŸ¥è¾“å‡º
            if torch.isnan(output).any():
                print(f"  âœ— è¾“å‡ºåŒ…å«NaN")
                return False
            
            if torch.isinf(output).any():
                print(f"  âœ— è¾“å‡ºåŒ…å«Inf")
                return False
            
            # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡
            local_weights = intermediate['local_attn_scores']
            global_weights = intermediate['global_attn_weights']
            
            if torch.isnan(local_weights).any() or torch.isnan(global_weights).any():
                print(f"  âœ— æ³¨æ„åŠ›æƒé‡åŒ…å«NaN")
                return False
            
            # æ£€æŸ¥softmaxè¾“å‡ºçš„æ•°å€¼èŒƒå›´
            local_softmax = F.softmax(local_weights, dim=-1)
            global_softmax = F.softmax(global_weights, dim=-1)
            
            if not (local_softmax.min() >= 0 and local_softmax.max() <= 1):
                print(f"  âœ— æœ¬åœ°softmaxè¾“å‡ºèŒƒå›´é”™è¯¯")
                return False
            
            if not (global_softmax.min() >= 0 and global_softmax.max() <= 1):
                print(f"  âœ— å…¨å±€softmaxè¾“å‡ºèŒƒå›´é”™è¯¯")
                return False
            
            print(f"  âœ“ {case_name} æ•°å€¼ç¨³å®š")
            
        except Exception as e:
            print(f"  âœ— {case_name} å¼•å‘å¼‚å¸¸: {e}")
            return False
    
    print("âœ“ æ•°å€¼ç¨³å®šæ€§æµ‹è¯•é€šè¿‡")
    return True


def test_transformer_block_integration():
    """æµ‹è¯•ä¸TransformerBlockçš„é›†æˆ"""
    print("\n=== æµ‹è¯•TransformerBlocké›†æˆ ===")
    
    # åˆ›å»ºå¯ç”¨ç¨€ç–æ³¨æ„åŠ›çš„é…ç½®
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=512,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6,
        use_sparse_attention=True
    )
    
    # åˆ›å»ºTransformerBlock
    block = TransformerBlock(gpt_config)
    
    # éªŒè¯ä½¿ç”¨äº†ç¨€ç–æ³¨æ„åŠ›
    from app.models.transformer.sparse_attention import SparseAttention
    if not isinstance(block.attn, SparseAttention):
        print("âœ— TransformerBlockæœªä½¿ç”¨SparseAttention")
        return False
    
    print("âœ“ TransformerBlockæ­£ç¡®ä½¿ç”¨SparseAttention")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    x = torch.randn(2, 32, gpt_config.n_embed)
    
    # ä¸è¿”å›ä¸­é—´å¼ é‡
    output, cache, intermediate = block(x, use_cache=False, return_intermediate=False)
    
    if output.shape != (2, 32, gpt_config.n_embed):
        print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}")
        return False
    
    # è¿”å›ä¸­é—´å¼ é‡
    output, cache, intermediate = block(x, use_cache=False, return_intermediate=True)
    
    if intermediate is None:
        print("âœ— æœªè¿”å›ä¸­é—´å¼ é‡")
        return False
    
    print("âœ“ TransformerBlocké›†æˆæµ‹è¯•é€šè¿‡")
    return True


def test_model_integration():
    """æµ‹è¯•ä¸å®Œæ•´æ¨¡å‹çš„é›†æˆ"""
    print("\n=== æµ‹è¯•å®Œæ•´æ¨¡å‹é›†æˆ ===")
    
    # åˆ›å»ºå¯ç”¨ç¨€ç–æ³¨æ„åŠ›çš„é…ç½®
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=256,
        n_layer=2,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6,
        use_sparse_attention=True
    )
    
    from app.models.transformer import GPT2Model
    
    model = GPT2Model(gpt_config)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.get_num_parameters():,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_ids = torch.randint(0, gpt_config.vocab_size, (2, 32))
    
    # ä¸è¿”å›ä¸­é—´å¼ é‡
    result = model(input_ids, use_cache=False, return_cache=False, return_intermediate=False)
    
    expected_shape = (2, 32, gpt_config.vocab_size)
    if result["logits"].shape != expected_shape:
        print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {result['logits'].shape}")
        return False
    
    # è¿”å›ä¸­é—´å¼ é‡
    result = model(input_ids, use_cache=False, return_cache=False, return_intermediate=True)
    
    if "intermediate" not in result:
        print("âœ— æ¨¡å‹æœªè¿”å›ä¸­é—´å¼ é‡")
        return False
    
    if len(result["intermediate"]) != gpt_config.n_layer:
        print(f"âœ— ä¸­é—´å¼ é‡å±‚æ•°é”™è¯¯: æœŸæœ› {gpt_config.n_layer}, å®é™… {len(result['intermediate'])}")
        return False
    
    print(f"  âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {result['logits'].shape}")
    print(f"  âœ“ ä¸­é—´å¼ é‡å±‚æ•°æ­£ç¡®: {len(result['intermediate'])}")
    
    print("âœ“ å®Œæ•´æ¨¡å‹é›†æˆæµ‹è¯•é€šè¿‡")
    return True


def test_comparison_with_standard_attention():
    """ä¸æ ‡å‡†æ³¨æ„åŠ›çš„å¯¹æ¯”æµ‹è¯•"""
    print("\n=== ä¸æ ‡å‡†æ³¨æ„åŠ›å¯¹æ¯”æµ‹è¯• ===")
    
    # åˆ›å»ºç›¸åŒé…ç½®çš„ä¸¤ä¸ªæ¨¡å‹
    gpt_config = GPT2Config(
        vocab_size=1000,
        context_size=256,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6
    )
    
    # æ ‡å‡†æ³¨æ„åŠ›æ¨¡å‹
    standard_config = GPT2Config(
        vocab_size=1000,
        context_size=256,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6,
        use_sparse_attention=False
    )
    
    # ç¨€ç–æ³¨æ„åŠ›æ¨¡å‹
    sparse_config = GPT2Config(
        vocab_size=1000,
        context_size=256,
        n_layer=1,
        n_embed=240,  # 240èƒ½è¢«6æ•´é™¤
        n_head=6,
        use_sparse_attention=True
    )
    
    from app.models.transformer import GPT2Model
    
    standard_model = GPT2Model(standard_config)
    sparse_model = GPT2Model(sparse_config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_ids = torch.randint(0, gpt_config.vocab_size, (2, 64))
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        standard_result = standard_model(input_ids)
        sparse_result = sparse_model(input_ids)
    
    # æ¯”è¾ƒè¾“å‡ºå½¢çŠ¶
    if standard_result["logits"].shape != sparse_result["logits"].shape:
        print("âœ— æ ‡å‡†æ³¨æ„åŠ›å’Œç¨€ç–æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶ä¸åŒ")
        return False
    
    print(f"âœ“ è¾“å‡ºå½¢çŠ¶ä¸€è‡´: {standard_result['logits'].shape}")
    
    # æ¯”è¾ƒè¾“å‡ºæ•°å€¼èŒƒå›´ï¼ˆåº”è¯¥å¤§è‡´ç›¸ä¼¼ï¼‰
    std_mean = standard_result["logits"].mean().item()
    std_std = standard_result["logits"].std().item()
    
    sparse_mean = sparse_result["logits"].mean().item()
    sparse_std = sparse_result["logits"].std().item()
    
    print(f"æ ‡å‡†æ³¨æ„åŠ›: mean={std_mean:.4f}, std={std_std:.4f}")
    print(f"ç¨€ç–æ³¨æ„åŠ›: mean={sparse_mean:.4f}, std={sparse_std:.4f}")
    
    # æ£€æŸ¥æ•°å€¼èŒƒå›´æ˜¯å¦åˆç†
    if abs(std_mean - sparse_mean) > 1.0 or abs(std_std - sparse_std) > 1.0:
        print("âš  è¾“å‡ºåˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œä½†è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„")
    else:
        print("âœ“ è¾“å‡ºåˆ†å¸ƒç›¸ä¼¼")
    
    print("âœ“ ä¸æ ‡å‡†æ³¨æ„åŠ›å¯¹æ¯”æµ‹è¯•é€šè¿‡")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•ç¨€ç–æ³¨æ„åŠ›æ¨¡å—...")
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        tests = [
            test_sparse_attention_config,
            test_sparse_attention_initialization,
            test_dynamic_window_size,
            test_mask_generation,
            test_sparse_attention_forward,
            test_sparsity_characteristics,
            test_numerical_stability,
            test_transformer_block_integration,
            test_model_integration,
            test_comparison_with_standard_attention,
        ]
        
        passed = 0
        total = len(tests)
        
        for test_func in tests:
            if test_func():
                passed += 1
            else:
                print(f"\nâŒ æµ‹è¯•å¤±è´¥: {test_func.__name__}")
                return False
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼({passed}/{total})")
        print("\nâœ“ ç¨€ç–æ³¨æ„åŠ›æ¨¡å—å®ç°ç‰¹ç‚¹:")
        print("  - åˆ†ç»„å¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå±€éƒ¨å¤´ + å…¨å±€å¤´ï¼‰")
        print("  - åŠ¨æ€å±€éƒ¨ç¨€ç–æ¨¡å¼ï¼ˆè‡ªé€‚åº”çª—å£å¤§å°ï¼‰")
        print("  - ä»…ä½¿ç”¨PyTorchæ“ä½œï¼Œæ— éœ€CUDAç‰¹åˆ¶æ ¸å¿ƒ")
        print("  - å®Œæ•´çš„ä¸­é—´å¼ é‡è¿”å›æœºåˆ¶")
        print("  - æ•°å€¼ç¨³å®šæ€§ä¿è¯ï¼ˆ-inf maskå¡«å……ï¼‰")
        print("  - ä¸ç°æœ‰Transformeræ¶æ„å®Œå…¨å…¼å®¹")
        print("  - è¯¦ç»†çš„å•å…ƒæµ‹è¯•è¦†ç›–")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)