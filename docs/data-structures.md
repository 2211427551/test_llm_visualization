# æ•°æ®ç»“æ„è¯´æ˜æ–‡æ¡£

## ğŸ“– æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†é¡¹ç›®ä¸­ä½¿ç”¨çš„å„ç§æ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬é…ç½®ç±»ã€æ•°æ®æ¨¡å¼ã€ä¸­é—´å¼ é‡æ ¼å¼ç­‰ã€‚ç†è§£è¿™äº›æ•°æ®ç»“æ„å¯¹äºæ­£ç¡®ä½¿ç”¨å’Œæ‰©å±•é¡¹ç›®åŠŸèƒ½è‡³å…³é‡è¦ã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ•°æ®ç»“æ„

### 1. æ¨¡å‹é…ç½®ç±»

#### GPT2Config

```python
@dataclass
class GPT2Config:
    """
    GPT-2é£æ ¼Transformeræ¨¡å‹çš„é…ç½®ç±»
    
    åŒ…å«æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰è¶…å‚æ•°ï¼Œæ”¯æŒç¨€ç–æ³¨æ„åŠ›å’ŒMoEç­‰é«˜çº§åŠŸèƒ½
    """
    # åŸºç¡€æ¨¡å‹é…ç½®
    vocab_size: int = 50304          # è¯è¡¨å¤§å°ï¼Œé€šå¸¸æ˜¯2çš„å¹‚æ¬¡
    context_size: int = 1024         # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    n_layer: int = 12                # Transformerå±‚æ•°
    n_head: int = 12                 # æ³¨æ„åŠ›å¤´æ•°
    n_embed: int = 768               # åµŒå…¥ç»´åº¦
    
    # è®­ç»ƒç›¸å…³é…ç½®
    dropout: float = 0.1             # Dropoutæ¦‚ç‡
    bias: bool = True                # æ˜¯å¦ä½¿ç”¨åç½®é¡¹
    
    # å‰é¦ˆç½‘ç»œé…ç½®
    ffn_hidden_multiplier: int = 4   # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦å€æ•°
    
    # ç¨€ç–æ³¨æ„åŠ›é…ç½®
    use_sparse_attention: bool = False  # ç¨€ç–æ³¨æ„åŠ›å¼€å…³
    
    # MoEé…ç½®
    use_moe: bool = False              # MoEå¼€å…³
    moe_num_experts: int = 8           # ä¸“å®¶æ•°é‡
    moe_top_k: int = 2                 # MoEè·¯ç”±top-k
    moe_activation: str = "gelu"       # MoEä¸“å®¶æ¿€æ´»å‡½æ•°
    moe_dropout: Optional[float] = None # MoEä¸“ç”¨dropout
```

**é…ç½®éªŒè¯æµç¨‹ï¼š**

```mermaid
graph TD
    A[åˆ›å»ºé…ç½®] --> B{éªŒè¯åµŒå…¥ç»´åº¦}
    B -->|n_embed % n_head == 0| C{éªŒè¯MoEé…ç½®}
    B -->|ä¸æ•´é™¤| D[æŠ›å‡ºValueError]
    C -->|use_moe=False| E[é…ç½®å®Œæˆ]
    C -->|use_moe=True| F{æ£€æŸ¥ä¸“å®¶æ•°é‡}
    F -->|moe_top_k <= moe_num_experts| G{æ£€æŸ¥æ¿€æ´»å‡½æ•°}
    F -->|top_k > ä¸“å®¶æ•°| H[æŠ›å‡ºValueError]
    G -->|æœ‰æ•ˆæ¿€æ´»å‡½æ•°| I{æ£€æŸ¥dropoutèŒƒå›´}
    G -->|æ— æ•ˆå‡½æ•°| J[æŠ›å‡ºValueError]
    I -->|0 <= dropout < 1| E
    I -->|èŒƒå›´é”™è¯¯| K[æŠ›å‡ºValueError]
```

#### SparseAttentionConfig

```python
@dataclass
class SparseAttentionConfig:
    """ç¨€ç–æ³¨æ„åŠ›é…ç½®"""
    # åˆ†ç»„é…ç½®
    local_heads: int = 8              # å±€éƒ¨æ³¨æ„åŠ›å¤´æ•°
    global_heads: int = 4             # å…¨å±€æ³¨æ„åŠ›å¤´æ•°
    
    # ç¨€ç–æ¨¡å¼é…ç½®
    window_size: int = 128            # å±€éƒ¨çª—å£å¤§å°
    global_token_ratio: float = 0.1    # å…¨å±€tokenæ¯”ä¾‹
    
    # åŠ¨æ€é…ç½®
    adaptive_window: bool = True      # æ˜¯å¦è‡ªé€‚åº”çª—å£å¤§å°
    min_window_size: int = 32         # æœ€å°çª—å£å¤§å°
    max_window_size: int = 512        # æœ€å¤§çª—å£å¤§å°
    
    # æ•°å€¼ç¨³å®šæ€§
    mask_value: float = -1e9         # maskå¡«å……å€¼
```

### 2. APIæ•°æ®æ¨¡å¼

#### è¯·æ±‚æ¨¡å¼

##### InitializeRequest

```python
class InitializeRequest(BaseModel):
    """æ¨¡å‹åˆå§‹åŒ–è¯·æ±‚"""
    config: Optional[str] = Field(
        default=None,
        description="æ¨¡å‹é…ç½®çš„JSONå­—ç¬¦ä¸²"
    )
```

##### ForwardRequest

```python
class ForwardRequest(BaseModel):
    """å‰å‘ä¼ æ’­è¯·æ±‚"""
    text: str = Field(..., description="è¾“å…¥æ–‡æœ¬")
    capture_data: bool = Field(
        default=False,
        description="æ˜¯å¦æ•è·ä¸­é—´æ•°æ®"
    )
    max_length: Optional[int] = Field(
        default=None,
        description="æœ€å¤§åºåˆ—é•¿åº¦é™åˆ¶"
    )
```

#### å“åº”æ¨¡å¼

##### InitializeResponse

```python
class InitializeResponse(BaseModel):
    """æ¨¡å‹åˆå§‹åŒ–å“åº”"""
    success: bool = Field(..., description="åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    config: Dict[str, Any] = Field(..., description="æ¨¡å‹é…ç½®ä¿¡æ¯")
```

##### ForwardResponse

```python
class ForwardResponse(BaseModel):
    """å‰å‘ä¼ æ’­å“åº”"""
    success: bool = Field(..., description="æ¨ç†æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    logits_shape: List[int] = Field(..., description="è¾“å‡ºlogitsçš„å½¢çŠ¶")
    sequence_length: int = Field(..., description="è¾“å…¥åºåˆ—é•¿åº¦")
    captured_data: Optional[Dict[str, Any]] = Field(
        default=None,
        description="æ•è·çš„ä¸­é—´æ•°æ®"
    )
    processing_time: Optional[float] = Field(
        default=None,
        description="å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰"
    )
```

### 3. ä¸­é—´æ•°æ®ç»“æ„

#### ç¨€ç–æ³¨æ„åŠ›ä¸­é—´æ•°æ®

```python
SparseAttentionIntermediate = {
    'local_mask': torch.Tensor,          # (n_heads, seq_len, seq_len) å±€éƒ¨æ³¨æ„åŠ›æ©ç 
    'global_mask': torch.Tensor,         # (n_heads, seq_len, seq_len) å…¨å±€æ³¨æ„åŠ›æ©ç 
    'window_size': torch.Tensor,         # (1,) åŠ¨æ€çª—å£å¤§å°
    'sparsity_ratio': torch.Tensor,      # (1,) ç¨€ç–æ¯”ä¾‹
    'local_attn_scores': torch.Tensor,   # (batch, local_heads, seq_len, seq_len) å±€éƒ¨æ³¨æ„åŠ›åˆ†æ•°
    'global_attn_scores': torch.Tensor,   # (batch, global_heads, seq_len, seq_len) å…¨å±€æ³¨æ„åŠ›åˆ†æ•°
    'global_tokens': torch.Tensor,       # (batch, seq_len) å…¨å±€tokenç´¢å¼•
}
```

**æ•°æ®æµç¨‹å›¾ï¼š**

```mermaid
graph LR
    A[è¾“å…¥åºåˆ—] --> B[è®¡ç®—çª—å£å¤§å°]
    B --> C[ç”Ÿæˆå±€éƒ¨æ©ç ]
    B --> D[é€‰æ‹©å…¨å±€token]
    D --> E[ç”Ÿæˆå…¨å±€æ©ç ]
    C --> F[å±€éƒ¨æ³¨æ„åŠ›è®¡ç®—]
    E --> G[å…¨å±€æ³¨æ„åŠ›è®¡ç®—]
    F --> H[æ³¨æ„åŠ›æƒé‡åˆå¹¶]
    G --> H
    H --> I[è¾“å‡ºä¸­é—´æ•°æ®]
```

#### MoEä¸­é—´æ•°æ®

```python
MoEIntermediate = {
    'gate_scores': torch.Tensor,         # (batch, seq_len, num_experts) æ‰€æœ‰ä¸“å®¶çš„gatingåˆ†æ•°
    'top_k_scores': torch.Tensor,        # (batch, seq_len, top_k) é€‰ä¸­çš„ä¸“å®¶åˆ†æ•°
    'top_k_indices': torch.Tensor,       # (batch, seq_len, top_k) é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
    'expert_outputs': List[Dict],        # æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºè¯¦æƒ…
    'final_output': torch.Tensor,         # (batch, seq_len, n_embed) æœ€ç»ˆåŠ æƒè¾“å‡º
    'load_balance_loss': torch.Tensor,   # (1,) è´Ÿè½½å‡è¡¡æŸå¤±
    'expert_usage': torch.Tensor,        # (num_experts,) ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
}
```

**ä¸“å®¶è¾“å‡ºè¯¦æƒ…ç»“æ„ï¼š**

```python
ExpertOutput = {
    'expert_idx': int,                   # ä¸“å®¶ç´¢å¼•
    'input_tokens': torch.Tensor,        # (num_tokens, n_embed) è¾“å…¥token
    'output_tokens': torch.Tensor,       # (num_tokens, n_embed) è¾“å‡ºtoken
    'weights': torch.Tensor,             # (num_tokens,) åŠ æƒæƒé‡
    'token_indices': torch.Tensor,       # (num_tokens,) tokenåœ¨åºåˆ—ä¸­çš„ç´¢å¼•
}
```

#### å®Œæ•´ä¸­é—´æ•°æ®ç»“æ„

```python
ModelIntermediate = {
    'embeddings': {
        'token_embeddings': torch.Tensor,    # (batch, seq_len, n_embed) tokenåµŒå…¥
        'position_embeddings': torch.Tensor, # (batch, seq_len, n_embed) ä½ç½®åµŒå…¥
        'combined_embeddings': torch.Tensor, # (batch, seq_len, n_embed) ç»„åˆåµŒå…¥
    },
    'layers': List[Dict],                  # æ¯å±‚çš„ä¸­é—´æ•°æ®
    'sparse_attention': SparseAttentionIntermediate,  # ç¨€ç–æ³¨æ„åŠ›æ•°æ®
    'moe': MoEIntermediate,                # MoEæ•°æ®
    'performance': {
        'forward_time': float,             # å‰å‘ä¼ æ’­æ—¶é—´
        'memory_usage': float,             # å†…å­˜ä½¿ç”¨é‡
        'layer_times': List[float]         # æ¯å±‚å¤„ç†æ—¶é—´
    }
}
```

## ğŸ“Š å¼ é‡å½¢çŠ¶è§„èŒƒ

### 1. è¾“å…¥è¾“å‡ºå¼ é‡

| å¼ é‡åç§° | å½¢çŠ¶ | æ•°æ®ç±»å‹ | è¯´æ˜ |
|----------|------|----------|------|
| input_ids | (batch_size, seq_len) | torch.long | è¾“å…¥token ID |
| attention_mask | (batch_size, seq_len) | torch.bool | æ³¨æ„åŠ›æ©ç  |
| embeddings | (batch_size, seq_len, n_embed) | torch.float | è¯åµŒå…¥ |
| logits | (batch_size, seq_len, vocab_size) | torch.float | è¾“å‡ºlogits |
| hidden_states | (batch_size, seq_len, n_embed) | torch.float | éšè—çŠ¶æ€ |

### 2. æ³¨æ„åŠ›ç›¸å…³å¼ é‡

| å¼ é‡åç§° | å½¢çŠ¶ | æ•°æ®ç±»å‹ | è¯´æ˜ |
|----------|------|----------|------|
| q_proj | (batch_size, seq_len, n_embed) | torch.float | æŸ¥è¯¢æŠ•å½± |
| k_proj | (batch_size, seq_len, n_embed) | torch.float | é”®æŠ•å½± |
| v_proj | (batch_size, seq_len, n_embed) | torch.float | å€¼æŠ•å½± |
| q | (batch_size, n_head, seq_len, head_dim) | torch.float | é‡å¡‘åçš„æŸ¥è¯¢ |
| k | (batch_size, n_head, seq_len, head_dim) | torch.float | é‡å¡‘åçš„é”® |
| v | (batch_size, n_head, seq_len, head_dim) | torch.float | é‡å¡‘åçš„å€¼ |
| attn_weights | (batch_size, n_head, seq_len, seq_len) | torch.float | æ³¨æ„åŠ›æƒé‡ |
| attn_output | (batch_size, seq_len, n_embed) | torch.float | æ³¨æ„åŠ›è¾“å‡º |

### 3. MoEç›¸å…³å¼ é‡

| å¼ é‡åç§° | å½¢çŠ¶ | æ•°æ®ç±»å‹ | è¯´æ˜ |
|----------|------|----------|------|
| gate_scores | (batch_size, seq_len, num_experts) | torch.float | Gatingåˆ†æ•° |
| top_k_scores | (batch_size, seq_len, top_k) | torch.float | Top-kåˆ†æ•° |
| top_k_indices | (batch_size, seq_len, top_k) | torch.long | Top-kç´¢å¼• |
| expert_input | (num_tokens, n_embed) | torch.float | ä¸“å®¶è¾“å…¥ |
| expert_output | (num_tokens, n_embed) | torch.float | ä¸“å®¶è¾“å‡º |

## ğŸ”„ æ•°æ®æµè½¬æ¢

### 1. æ–‡æœ¬åˆ°å¼ é‡è½¬æ¢æµç¨‹

```mermaid
graph TD
    A[åŸå§‹æ–‡æœ¬] --> B[åˆ†è¯å¤„ç†]
    B --> C[Token IDåºåˆ—]
    C --> D[å¼ é‡åŒ–]
    D --> E[æ·»åŠ æ‰¹æ¬¡ç»´åº¦]
    E --> F[ç”Ÿæˆæ³¨æ„åŠ›æ©ç ]
    F --> G[æ¨¡å‹è¾“å…¥]
    
    G --> H[åµŒå…¥å±‚]
    H --> I[Transformerå±‚]
    I --> J[è¾“å‡ºæŠ•å½±]
    J --> K[Logitsè¾“å‡º]
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
def text_to_tensors(text: str, tokenizer, max_length: int = 512):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥å¼ é‡"""
    # 1. åˆ†è¯
    tokens = tokenizer.encode(text)
    
    # 2. æˆªæ–­æˆ–å¡«å……
    if len(tokens) > max_length:
        tokens = tokens[:max_length]
    else:
        tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))
    
    # 3. è½¬æ¢ä¸ºå¼ é‡
    input_ids = torch.tensor([tokens], dtype=torch.long)
    
    # 4. ç”Ÿæˆæ³¨æ„åŠ›æ©ç 
    attention_mask = (input_ids != tokenizer.pad_token_id)
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'original_length': len(tokenizer.encode(text))
    }
```

### 2. ä¸­é—´æ•°æ®åºåˆ—åŒ–

```python
def serialize_intermediate(intermediate_data: Dict[str, Any]) -> Dict[str, Any]:
    """å°†ä¸­é—´æ•°æ®åºåˆ—åŒ–ä¸ºå¯ä¼ è¾“çš„æ ¼å¼"""
    serialized = {}
    
    for key, value in intermediate_data.items():
        if isinstance(value, torch.Tensor):
            # è½¬æ¢å¼ é‡ä¸ºåˆ—è¡¨
            serialized[key] = {
                'data': value.tolist(),
                'shape': list(value.shape),
                'dtype': str(value.dtype)
            }
        elif isinstance(value, dict):
            # é€’å½’å¤„ç†å­—å…¸
            serialized[key] = serialize_intermediate(value)
        elif isinstance(value, list):
            # å¤„ç†åˆ—è¡¨
            serialized[key] = [
                serialize_intermediate(item) if isinstance(item, dict) else item
                for item in value
            ]
        else:
            # ç›´æ¥å¤åˆ¶å…¶ä»–ç±»å‹
            serialized[key] = value
    
    return serialized
```

### 3. æ•°æ®éªŒè¯å™¨

```python
class DataValidator:
    """æ•°æ®ç»“æ„éªŒè¯å™¨"""
    
    @staticmethod
    def validate_config(config: GPT2Config) -> List[str]:
        """éªŒè¯é…ç½®å¯¹è±¡"""
        errors = []
        
        # éªŒè¯åŸºç¡€é…ç½®
        if config.n_embed <= 0:
            errors.append("åµŒå…¥ç»´åº¦å¿…é¡»å¤§äº0")
        
        if config.n_head <= 0:
            errors.append("æ³¨æ„åŠ›å¤´æ•°å¿…é¡»å¤§äº0")
        
        if config.n_embed % config.n_head != 0:
            errors.append("åµŒå…¥ç»´åº¦å¿…é¡»èƒ½è¢«æ³¨æ„åŠ›å¤´æ•°æ•´é™¤")
        
        # éªŒè¯MoEé…ç½®
        if config.use_moe:
            if config.moe_num_experts <= 0:
                errors.append("MoEä¸“å®¶æ•°é‡å¿…é¡»å¤§äº0")
            
            if config.moe_top_k > config.moe_num_experts:
                errors.append("MoE top_kä¸èƒ½å¤§äºä¸“å®¶æ•°é‡")
        
        return errors
    
    @staticmethod
    def validate_tensors(tensors: Dict[str, torch.Tensor]) -> List[str]:
        """éªŒè¯å¼ é‡æ•°æ®"""
        errors = []
        
        required_keys = ['input_ids', 'attention_mask']
        for key in required_keys:
            if key not in tensors:
                errors.append(f"ç¼ºå°‘å¿…éœ€çš„å¼ é‡: {key}")
        
        # éªŒè¯å½¢çŠ¶åŒ¹é…
        if 'input_ids' in tensors and 'attention_mask' in tensors:
            if tensors['input_ids'].shape != tensors['attention_mask'].shape:
                errors.append("input_idså’Œattention_maskå½¢çŠ¶ä¸åŒ¹é…")
        
        return errors
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§æ•°æ®

### 1. æ€§èƒ½æŒ‡æ ‡ç»“æ„

```python
PerformanceMetrics = {
    'timing': {
        'total_time': float,              # æ€»å¤„ç†æ—¶é—´
        'embedding_time': float,          # åµŒå…¥å±‚æ—¶é—´
        'layer_times': List[float],       # æ¯å±‚å¤„ç†æ—¶é—´
        'output_time': float,              # è¾“å‡ºå±‚æ—¶é—´
    },
    'memory': {
        'peak_memory': float,             # å³°å€¼å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
        'layer_memory': List[float],      # æ¯å±‚å†…å­˜ä½¿ç”¨
        'gradient_memory': float,         # æ¢¯åº¦å†…å­˜ä½¿ç”¨
    },
    'throughput': {
        'tokens_per_second': float,       # æ¯ç§’å¤„ç†çš„tokenæ•°
        'batch_throughput': float,        # æ‰¹å¤„ç†ååé‡
    },
    'model_stats': {
        'total_parameters': int,          # æ€»å‚æ•°é‡
        'active_parameters': int,         # æ´»è·ƒå‚æ•°é‡ï¼ˆMoEï¼‰
        'flops_per_token': int,           # æ¯ä¸ªtokençš„æµ®ç‚¹è¿ç®—æ•°
    }
}
```

### 2. ç›‘æ§æ•°æ®æ”¶é›†

```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = PerformanceMetrics()
        self.start_time = None
        self.memory_tracker = []
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        torch.cuda.reset_peak_memory_stats()
    
    def record_layer_time(self, layer_idx: int, duration: float):
        """è®°å½•å±‚å¤„ç†æ—¶é—´"""
        self.metrics['timing']['layer_times'].append({
            'layer_idx': layer_idx,
            'duration': duration
        })
    
    def record_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨"""
        if torch.cuda.is_available():
            current_memory = torch.cuda.memory_allocated() / 1024**2  # MB
            peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            self.memory_tracker.append({
                'timestamp': time.time(),
                'current_memory': current_memory,
                'peak_memory': peak_memory
            })
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        total_time = time.time() - self.start_time if self.start_time else 0
        
        return {
            'total_time': total_time,
            'peak_memory': max(m['peak_memory'] for m in self.memory_tracker),
            'average_layer_time': np.mean([l['duration'] for l in self.metrics['timing']['layer_times']]),
            'tokens_per_second': self.calculate_throughput()
        }
```

## ğŸ”§ å·¥å…·å‡½æ•°

### 1. æ•°æ®è½¬æ¢å·¥å…·

```python
class DataConverter:
    """æ•°æ®è½¬æ¢å·¥å…·ç±»"""
    
    @staticmethod
    def config_to_dict(config: GPT2Config) -> Dict[str, Any]:
        """å°†é…ç½®å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'vocab_size': config.vocab_size,
            'context_size': config.context_size,
            'n_layer': config.n_layer,
            'n_head': config.n_head,
            'n_embed': config.n_embed,
            'dropout': config.dropout,
            'bias': config.bias,
            'ffn_hidden_multiplier': config.ffn_hidden_multiplier,
            'use_sparse_attention': config.use_sparse_attention,
            'use_moe': config.use_moe,
            'moe_num_experts': config.moe_num_experts,
            'moe_top_k': config.moe_top_k,
            'moe_activation': config.moe_activation,
            'moe_dropout': config.moe_dropout,
        }
    
    @staticmethod
    def dict_to_config(config_dict: Dict[str, Any]) -> GPT2Config:
        """ä»å­—å…¸åˆ›å»ºé…ç½®å¯¹è±¡"""
        return GPT2Config(**config_dict)
    
    @staticmethod
    def tensors_to_numpy(tensors: Dict[str, torch.Tensor]) -> Dict[str, np.ndarray]:
        """å°†å¼ é‡è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        return {k: v.cpu().numpy() for k, v in tensors.items()}
    
    @staticmethod
    def numpy_to_tensors(arrays: Dict[str, np.ndarray]) -> Dict[str, torch.Tensor]:
        """å°†numpyæ•°ç»„è½¬æ¢ä¸ºå¼ é‡"""
        return {k: torch.from_numpy(v) for k, v in arrays.items()}
```

### 2. æ•°æ®éªŒè¯å·¥å…·

```python
class DataValidator:
    """æ•°æ®éªŒè¯å·¥å…·"""
    
    @staticmethod
    def validate_tensor_shape(tensor: torch.Tensor, expected_shape: Tuple[int, ...]) -> bool:
        """éªŒè¯å¼ é‡å½¢çŠ¶"""
        return tensor.shape == expected_shape
    
    @staticmethod
    def validate_tensor_range(tensor: torch.Tensor, min_val: float, max_val: float) -> bool:
        """éªŒè¯å¼ é‡æ•°å€¼èŒƒå›´"""
        return tensor.min() >= min_val and tensor.max() <= max_val
    
    @staticmethod
    def validate_no_nan_inf(tensor: torch.Tensor) -> bool:
        """éªŒè¯å¼ é‡æ— NaNæˆ–Inf"""
        return not (torch.isnan(tensor).any() or torch.isinf(tensor).any())
    
    @staticmethod
    def validate_attention_weights(attn_weights: torch.Tensor) -> bool:
        """éªŒè¯æ³¨æ„åŠ›æƒé‡"""
        # æ£€æŸ¥å½¢çŠ¶
        if len(attn_weights.shape) != 4:
            return False
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        if not DataValidator.validate_tensor_range(attn_weights, 0.0, 1.0):
            return False
        
        # æ£€æŸ¥æ¯è¡Œå’Œä¸º1ï¼ˆå…è®¸å°çš„æ•°å€¼è¯¯å·®ï¼‰
        row_sums = attn_weights.sum(dim=-1)
        if not torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6):
            return False
        
        return True
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ•°æ®ç»“æ„è®¾è®¡åŸåˆ™

1. **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨Pydanticè¿›è¡Œæ•°æ®éªŒè¯
2. **æ–‡æ¡£å®Œæ•´**ï¼šä¸ºæ¯ä¸ªå­—æ®µæä¾›è¯¦ç»†çš„æè¿°
3. **å‘åå…¼å®¹**ï¼šæ–°ç‰ˆæœ¬ä¿æŒå‘åå…¼å®¹æ€§
4. **åºåˆ—åŒ–å‹å¥½**ï¼šæ”¯æŒJSONåºåˆ—åŒ–å’Œååºåˆ—åŒ–
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šé¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶

### 2. é”™è¯¯å¤„ç†ç­–ç•¥

```python
class DataStructureError(Exception):
    """æ•°æ®ç»“æ„ç›¸å…³é”™è¯¯"""
    pass

class ValidationError(DataStructureError):
    """æ•°æ®éªŒè¯é”™è¯¯"""
    pass

class ShapeError(DataStructureError):
    """å¼ é‡å½¢çŠ¶é”™è¯¯"""
    pass

def safe_data_conversion(data: Any, target_type: str) -> Any:
    """å®‰å…¨çš„æ•°æ®è½¬æ¢"""
    try:
        if target_type == "tensor":
            return torch.tensor(data)
        elif target_type == "numpy":
            return np.array(data)
        elif target_type == "config":
            return GPT2Config(**data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡ç±»å‹: {target_type}")
    except Exception as e:
        raise DataStructureError(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
```

### 3. è°ƒè¯•å·¥å…·

```python
def debug_data_structure(data: Any, name: str = "data", max_depth: int = 3):
    """è°ƒè¯•æ•°æ®ç»“æ„"""
    def _debug(obj, depth=0):
        indent = "  " * depth
        
        if depth >= max_depth:
            print(f"{indent}{name}: {type(obj)} (max depth reached)")
            return
        
        if isinstance(obj, dict):
            print(f"{indent}{name}: dict with {len(obj)} items")
            for k, v in obj.items():
                _debug(v, f"{name}.{k}", depth + 1)
        elif isinstance(obj, list):
            print(f"{indent}{name}: list with {len(obj)} items")
            if len(obj) > 0:
                _debug(obj[0], f"{name}[0]", depth + 1)
        elif isinstance(obj, torch.Tensor):
            print(f"{indent}{name}: Tensor {obj.shape} {obj.dtype}")
        elif isinstance(obj, np.ndarray):
            print(f"{indent}{name}: Array {obj.shape} {obj.dtype}")
        else:
            print(f"{indent}{name}: {type(obj)} = {str(obj)[:100]}")
    
    _debug(data)
```

---

ğŸ’¡ **æç¤º**ï¼šåœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå»ºè®®ä½¿ç”¨ä¸Šè¿°è°ƒè¯•å·¥å…·æ¥éªŒè¯æ•°æ®ç»“æ„çš„æ­£ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„åµŒå¥—æ•°æ®æ—¶ã€‚